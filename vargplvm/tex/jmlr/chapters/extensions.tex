
%----------------------------------------------- EXTENSIONS -----------------------------------------------------
\section{\label{section:extensions} Extensions}
%----------------------------------------------- LEARNING FROM MULTIPLE SEQUENCES -----------------------------------------
\subsection{Learning from Multiple Sequences \label{sequences}}

A given data set of multivariate timeseries
may consist of a group of independent observed sequences, each with
a different length (e.g.\ in human motion capture data several walks
from a subject). Let, for example, the dataset be a group of
$S$ independent sequences  $\left( Y^{(1)}, ..., Y^{(S)} \right)$. We would like the dynamical version of our
model to capture the underlying
commonality of these data. We handle this by allowing a different temporal latent function for each of the independent
sequences, so that $X^{(s)}$ is the set of latent variables corresponding to the sequence $s$.
%so that there are different sets of latent variables $X^{(s)}$ within a shared latent space. 
%
These sets are a priori assumed to be independent since they correspond to separate sequences,
i.e.\ $p\left( X^{(1)}, X^{(2)}, ..., X^{(S)} \right) = \prod_{s=1}^S p(X^{(s)})$, where we dropped the
conditioning on time for simplicity.
%These sets are a priori assumed to be
%independent, i.e.\ $p\left( X^{(1)}, X^{(2)}, ..., X^{(S)} \right) = \prod_{s=1}^S p(X^{(s)})$, where we dropped the
%conditioning on time for simplicity.
%
This factorisation leads to a block-diagonal structure for the time covariance matrix $K_t$, where each block corresponds to one sequence.
 In this setting, each block of observations $Y^{(s)}$ is generated from its corresponding $X^{(s)}$
according to $Y^{(s)} = F^{(s)} + \boldsymbol \epsilon$, where the latent function which governs this mapping is shared across all sequences and 
$\boldsymbol \epsilon$ is Gaussian noise. 


%----------------------------------------------- HANDLING VERY HIGH DIMENSIONAL DATA -----------------------------------------
\subsection{Handling Very High Dimensional Datasets}

Our variational framework avoids the typical cubic complexity of
Gaussian processes allowing relatively large training sets (thousands
of time points, $N$). Further, the model scales only linearly with the
number of dimensions $D$. Specifically, the number of dimensions only
matters when performing calculations involving the data matrix $Y$. In
the final form of the lower bound (and consequently in all of the
derived quantities, such as gradients) this matrix only appears in the
form $Y Y^\T$ which can be precomputed. This means that, when $N \ll
D$, we can calculate $Y Y^\T$ only once and then substitute $Y$ with
the SVD (or Cholesky decomposition) of $Y Y^\T$. In this way, we can
work with an $N \times N$ instead of an $N \times D$
matrix. Practically speaking, this allows us to work with data sets
involving millions of features. In our experiments we model directly
the pixels of HD quality video, exploiting this trick.



%\section{Initialisation ...?}



%--------------------------------------- Gaussian process inference with uncertain inputs  -------------------
\subsection{Gaussian process inference with uncertain inputs\label{uncertainInputs}}

Gaussian processes have been used extensively and with great success in a variety of regression tasks.
In the most common setting, we are given a dataset of observed input-output pairs, 
denoted as $Z \in \mathbb{R}^{N \times Q}$ and $Y \in \mathbb{R}^{N \times D}$ respectively, 
and we wish to infer the unknown outputs $Y^* \in \mathbb{R}^{N^* \times D}$ corresponding to 
some novel given inputs $Z^* \in \mathbb{R}^{N^* \times Q}$. However, in many real-world applications
the inputs are uncertain, for example when measurements come from noisy sensors.
In this case, the GP methodology cannot be trivially extended to account for the variance
associated with the input space \cite{Girard:uncertain01, mchutchon:gaussian}. 
The aforementioned problem is also closely related to the field of heteroscedastic Gaussian process regression,
where the uncertainty in the noise levels is modelled in the output space as a function of the inputs 
\cite{Kersting:MLH07,Bishop:gps_nips97,gredilla:variationalheteroscedastic11}.

In this section we show our variational framework can be used to explicitly model the input uncertainty
in the GP regression setting. 
% Relate with other methods?
The assumption made is that the real inputs $Z$ are corrupted by Gaussian noise according to:
\begin{equation}
\label{uncertainInputsX}
 \bfx_n = \bfzi_n + \bfepsilon_x
\end{equation}
where $\bfzi_n$ denotes the $n$-th observed input of the dataset $Z$ and $\bfepsilon_z \sim \mathcal{N}(\bfzero, \Sigma_z)$,
as in \cite{mchutchon:gaussian}. However, in our case the noise can be correlated across inputs (or across dimensions),
\ie $\Sigma_z$ is not necessarily diagonal. In this setting, $X$ plays the role of a latent variable which is normally
distributed. Indeed, taking expectations of equation \eqref{uncertainInputsX}, reveals that
\begin{equation}
 \label{uncertainInputsPX}
p(X) = \prod_{n=1}^N \mathcal{N}(\bfx_n | \bfzi_n, \Sigma_z).
\end{equation}

Using the above prior in our variational framework means that we can define a variational bound on $p(Y)$ as well
as an associated approximation $q(X)$ to the true posterior $p(X|Y)$. This variational distribution $q(X)$ can be used
as a probability estimate of the noisy input locations $X$.
The expression of the lower bound in this case is identical to the one already defined in
section \ref{temporalPrior}, with the difference that the factorisation is now taken to be with respect to the datapoints
and that the following regularization term is now required to be added:
\begin{equation}
\sum_{n=1}^N \left[ \frac{1}{2} \tr \left(\Sigma_z^{-1} \bfzi \bfzi^\T \right) 
                   - \tr \left(\Sigma_x^{-1} \bfmu_n \bfzi^\T \right) \right] .
\end{equation}
The above term results from the new expression for the $\text{KL}$ divergence between the variational distribution
$q(X)$ and the prior $p(X)$ defined in equation \eqref{uncertainInputsPX}.
The factorisation can be equivalently be taken with respect to the dimensions of the latent space.
What is more, the prior distribution of equation \eqref{uncertainInputsPX} can be substituted
with a Gaussian process prior which has a constant mean $Z$.

In order to obtain a prediction $\bfx^*$ for a given test point $\bfzi^*$, we simply sample points
according to $\bfx^* \sim \mathcal{N}(\bfzi^*, \Sigma_z$). These samples can then be averaged
to get a single estimate.

\subsubsection{Autoregressive dynamics\label{autoregressive}}

Having a method which implicitly models the
uncertainty in the inputs also allows for doing predictions in an autoregressive manner while
propagating the uncertainty through the prediction sequence \cite{Girard:uncertain01}.
To demonstrate this in the context of our framework, we will take the simple case where the
process of interest is a multivariate timeseries given as pairs of time points $\bft = \{t\}_{n=1}^N$ and
corresponding output locations $Y = \{\bfy_n\}_{n=1}^N$. Here, we take the time locations to be
deterministic and equally spaced, so that the time locations can be denoted by the subscript of 
the output points $\bfy_n$. 

We can now reformat the given data $Y$ into input-output pairs $\hat{Z}$ and $\hat{Y}$,
where:
\begin{align*}
[\hat{\bfzi}_1, \hat{\bfzi}_2, ...] &= \left[ \left[\bfy_1, \bfy_2, ..., \bfy_k \right], \left[\bfy_2, \bfy_3, ..., \bfy_{k+1}\right], ...\right], \\
[\hat{\bfy}_1, \hat{\bfy}_2, ...] &= [\bfy_k, \bfy_{k+1}, ...]
\end{align*}
and $k$ is the size of the dynamics' ``memory''.
%%% TIE NOISE...
In other words, we define a window of size $k$ which shifts in time so that the outputs in time $t$ becomes an input
in time $t+1$. Therefore, the input and output noise of the assumed generative process can be tied into a single variable,
as in \cite{mchutchon:gaussian}, and the method described in section \ref{uncertainInputs}
can be applied to the new dataset $[\hat{Z}, \hat{Y}]$.



\subsubsection{Modelling temporal discontinuities with GPs}

One of the advantages of the proposed method is that it does not rely
on local approximation methods. Therefore, it is more suitable for modelling
processes with temporal discontinuities. More specifically, the variational
distribution $q(X)$, approximating $p(X|Y)$, is flexible enough to
interpolate between the ``jumps'' while, at the same time, being able to reconstruct the
training data (\ie the areas outside the ``jumps'').


\highlight{TODO} \textit{If there's not much more to say about it, this can be merged with the main section...}