
%---------------------------------- INTRODUCTION ------------------------------------------------------------------
\section{Introduction}
%\begin{verbatim}
%1. dimensionality reduction
%2. gplvm, applications, dynamics etc 
%3. training: so far, all of the above have been trained with MAP
%4. We present... . Advantages: prior on the latent space allows to capture assumptions and prior knowledge (e.g. dynamics),
% fully bayesian  approach resists overfitting, learn automatically the dimensionality of the latent space.
%* The remainder of this paper is structured as follows:
%
%
%1.
%---- Plan 1 
%dimensionality reduction to model a set of observations
%spectral vs probabilistic
%linear vs nonlinear
%gplvm
%gplvm extensions and applications (incl. dynamics etc, briefly).
%our contribution: a variational framework that allows for bayesian treatment of the gplvm including the dynamics extension
%
%--
%* Nonlinear dimensionality reduction:
%  Kernel PCA (non-probabilistic) etc
%* MCMC methods?
%* papers that use GPLVM with MAP
%... and applications.
%
%---- Plan 2
%Gaussian processes , regression ... ?
%
%-------------
%
%
%\end{verbatim}

%Modelling high dimensional datasets constitutes a key challenge for the machine learning community, since capturing
% the structure of the data is not easy in many dimensions.
High dimensional data are endemic in applications of machine learning.
  A typical starting point when dealing with such problems,
 is to assume that the data can be represented in a lower dimensional subspace immersed in the original, high dimensional
 one. Many successful methods (\eg PCA) seek a low dimensional manifold which is a linear subspace of the 
original data. However, this kind of linearity often constitutes a crude assumption for high dimensional and real world 
data. Here, we focus on non-linear dimensionality reduction methods.


Given a set of data, a large
range of non-probabilistic dimensionality reduction approaches have been
suggested, ranging from spectral methods such as KPCA \citep{Scholkopf:kernelpca97}, Isomap \citep{Tenenbaum:isomap00}
and other methods based on Multi-Dimensional Scaling \citep{Mardia:multivariate79}, to  iterative methods, such as Sammon mappings
\citep{Sammon:nonlinear69} and to other local distance preservation methods, such as LLE \citep{Roweis:lle00}.

Probabilistic approaches, such as GTM \citep{Bishop:gtm_ncomp98} and Density Networks \citep{MacKay:wondsa95}, view the dimensionality
reduction problem under a different perspective, since they seek a mapping from a low-dimensional latent space
to the observed data space, and come with certain advantages. More precisely, their generative nature and the forward mapping
that they define, allows them to be 
extended more easily in various ways (\eg with additional dynamics modelling), to be incorporated into a Bayesian 
framework for parameter learning and to handle more naturally missing data.

%often based on
%Multi-Dimensional Scaling [4], to generative methods such
%as GTM [3] and density networks. \cite{}.


%--
%\par In general, dimensionality reduction algorithms can be roughly classified as spectral or probabilistic. Spectral 
%approaches seek to represent the original $N$ datapoints by finding the eigendecomopsition of a (centered) similarity
% $N \times N$ matrix $K$ which summarizes the data structure. This is basically the main idea behind Classical 
%Multidimensional Scaling \cite{} which, under a certain viewpoint, can be seen as the root of many more recent 
%approaches \cite{} which mainly differ in the metric used to compute the similarity matrix or in the incorporation 
%of a mapping between the low and the high dimensional spaces. 
%%For example, Isomap \cite{} is differed from MDS in 
%%that the distance measure used seeks to preserve ... . Kernel PCA \cite{} .... . However, given that the eigenvectors
%% are never computed in the  ... real dimesinoality reductino is not always achieved.... . LLE .... . 
%\highlight{TODO:} Isomap, KPCA, LLE...

%\par
%% [In contrast, a probabilistic interpretation of dimensionality reduction problems results in models which have 
%%different (and for some applications more attractive) advantages.].
% Probabilistic approaches view the dimensionality
% reduction problem under a different perspective and come with certain advantages. More precisely, they can be 
%extended more easily in various ways (e.g. with additional dynamics modelling), can be incorporated into a Bayesian 
%framework for parameter learning and can handle more naturally missing data. \highlight{TODO:} GTM and Density networks ... .
%---


\par The Gaussian Process Latent Variable Model (GP-LVM) \citep{GPLVM} is a more recent probabilistic dimensionality 
reduction method which has been proven to be very robust for high dimensional problems 
\citep{gplvmLarger, Damianou:vgpds11}. GP-LVM can be seen 
as a non-linear generalisation of PPCA \citep{Tipping:probpca99}, which also has a
 Bayesian interpretation \citep{Bishop:bayesPCA98}. 
Unlike PPCA, though, the non-linear mapping of GP-LVM makes a Bayesian
treatment much more challenging.
Therefore, GP-LVM itself and all of its extensions, rely on a MAP training procedure.
 However, a principled Bayesian formulation is highly desirable, since it would allow for robust training of the model,
 automatic selection of the latent space's dimensionality as well as more intuitive exploration of the latent space's 
structure.

\par In this paper we formulate a variational inference framework which allows us to integrate out the inputs of the
Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The
procedure followed here is non-standard, as computation of a closed-form Jensen's lower bound on the true log marginal
 likelihood of the data is infeasible with standard variational inference. Instead, 
we build on, and significantly extend, the variational sparse GP method of \cite{Titsias:variational09}, where the GP prior is augmented to include auxiliary inducing variables so that the approximation is applied on an expanded probability model. 
\todo{expand a bit, talk also about the uncertain inputs and the flexibility of the model}


\par In the remainder of this paper we review the basics of GP-LVMs and its dynamical extensions,
 in section \ref{section:background}, and we proceed in presenting our variational framework and 
 Bayesian training procedure in section \ref{section:vgplvm}. In section \ref{section:predictions} we
 describe how the resulting model can be used for various kinds of predictive tasks and in section \ref{section:extensions}
we discuss natural but important extensions of our model. In section \ref{section:experiments}, we
 describe the experiments conducted on real world datasets and based on the results and
the analysis performed there, we present our final conclusions
in section \ref{section:conclusion}.

% \section{Related Work}

