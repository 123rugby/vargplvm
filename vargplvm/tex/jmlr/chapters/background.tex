
%---------------------------------- BACKGROUND ---------------------------------------------------------------------------------
\section{Gaussian Process Latent Variable Models \label{section:background}}
%---------------------------------- GP-LVM's -----------------------------------------------------------------------------------
%\subsection{Gaussian Process Latent Variable Models}

\subsection{GP Latent Variable Model and current MAP training approach}

The probabilistic or generative approach to dimensionality reduction
assumes that observed data $Y \in \mathbb{R}^{N \times D}$, where each row $\bfy_n$ is a data point, are generated by 
low dimensional latent variables $X \in \mathbb{R}^{N \times Q}$ ($Q \ll D$), where 
each row $\bfx_n$ is the latent vector associated with data point $\bfy_n$, via a mapping 
$\bff(\bfx) = (f_1 (\bfx),\ldots,f_D(\bfx))$:
\begin{equation}
\label{generative}
\bfy_n = \bff(\bfx_n) + \bfepsilon_n \ \  \text{or} \ \  y_{nd} = f_d(\bfx_n) + \epsilon_{nd},d=1,\ldots,D
\end{equation}
Here, $y_{nd}$ denotes the element from the $n$th row and $d$th column of $Y$ and
$\epsilon_{nd}$ is the additive noise term.
All quantities appearing on the right hand side of equation \eqref{generative}
are unknown, and we wish to learn them in a probabilistic manner. Therefore, we
employ suitable priors for the unknown quantities, so as to regularise the learning procedure.
These priors are directly associated with our set of modelling assumptions.

Alternative approaches to probabilistic dimensionality reduction 
make different assumptions regarding the form of the mapping function $f$ and the additive
noise. For instance, probabilistic PCA \citep{PPCA} considers a linear 
mapping and isotropic Gaussian noise while factor analysis allows for anisotropic Gaussian 
noise. The important ingredient of such methods is the linearilty assumption 
regarding the mapping, which however, can be unrealistic 
for several applications.

In this article, we are concerned with 
the Gaussian Process Latent Variable Model (GP-LVM) \citep{GPLVM, GPLVM2} which
targets to fully relax the linearity of the mapping in (\ref{generative})
and learn it in a flexible fully non-parametric way by using Gaussian processes.    
GP-LVM places a Gaussian Process (GP) \citep{rasmussen-williams}  
distribution on the mapping $\bff(\bfx)$ so that the type of non-linearity
is determined by a covariance or kernel function. 
More precisely, $\bff(\bfx)$ follows a multivariate Gaussian process indexed by 
$\bfx$ so that 
\begin{equation}
\label{gppriorf}
f_d(\bfx)  \sim  \mathcal{GP}(0, k_f(\bfx_i,\bfx_j)), \ \ d=1,\ldots,D.
\end{equation}
Here, the individual components of the latent function $\bff$ are taken to be independent draws from a Gaussian
process with covariance function $k_f(\bfx_i,\bfx_j)$, parametrised by a vector of parameters $\bftheta_f$. 
This covariance function determines the properties of the latent
mapping $\bff(\bfx)$ so when, for instance, a linear covariance function is used, 
GP-LVM becomes equivalent to traditional PPCA \citep{GPLVM}. However, when nonlinear
covariance functions are used, GP-LVM is able to perform non-linear dimensionality reduction. 
The covariance function considered in  \citep{GPLVM} is the squared exponential (RBF):
\begin{align}
k_{f(rbf)} \left( \mathbf{x}_i, \mathbf{x}_j \right) = {} &  
		\sigma_{rbf}^2 \exp\left(
			- \frac{1}{2 l^2} \sum_{q=1}^{Q} \left(
                          \mathit{x_{i,q} - x_{j,q}} \right) ^2 \right).
\label{rbf}
\end{align}
\noindent which is infinitely differentiable and uses a common lengthscale
 parameter $l$ for all latent dimensions.
% In this paper we 
%employ the square exponential covariance function which uses 
%a different lengthscale per input dimension, 
%and takes the form:
%\begin{align}
%k_{f(ard)}  \left( \mathbf{x}_i, \mathbf{x}_j \right) = {} &  
%		\sigma_{ard}^2 \exp \left(
%			- \frac{1}{2} \sum_{q=1}^{Q} w_q \left(
%                          \mathit{x_{i,q} - x_{j,q}} \right) ^2 \right).
%\label{ard}
%\end{align}
%%This covariance function enables an automatic relevance determination (ARD) procedure and the reason for opting for it
%%will become apparent later in this paper, when the Bayesian training framework for GP-LVM will be described.
%The reason for opting for the above covariance function is that the variational Bayesian training
%framework, presented in the next section, will enable an automatic relevance determination (ARD) procedure
%that automaticaly selects the latent dimensionality. 
\highlight{...} \\
Finally, as already discussed, a dual version of PPCA can be obtained by choosing a linear 
(ARD) covariance function:
\begin{equation}
\label{linard}
k_{f(lin)} \left( \bfx_i, \bfx_j \right) = \bfx^\T C \bfx,
\end{equation}
where $C$ is a positive definite diagonal covariance matrix whose diagonal element $C_{q,q}$ can be seen as the scale for the $q$th dimension.


\todo{Move the ARD kernel further down (eg in a ``critisism for MAP'' chapter'' and the ``white", ``bias'' in the experiments}

%Although most of the GPLVM-based approaches in the literature assume that $w_i=w_j, \foreach i,j \in \{1,...,Q\}$,
%in this paper we allow a different scale $w_q$ for each latent dimension. This, in combination with the Bayesian
%framework that will be demonstrated later in this paper, enables an automatic relevance
%determination procedure (ARD), i.e.\ it allows Bayesian training to ``switch off'' unnecessary dimensions by
%driving the values of the corresponding scales to zero. In that way, our models are able to find automatically an
%effective number of dimensions for the latent space.

\par Since the GPs are taken to be independent across the features, as is shown in equation \eqref{gppriorf}), we can write 
$p(F | X) = \prod_{d=1}^D p(\bff_d | X)$, where the matrix $F \in \mathbb{R}^{N \times D}$ (with columns
$\{ \bff_d \}_{d=1}^D$) denotes the mapping latent variables \ $f_{nd} =f_d(\bfx_n)$
associated with observations $Y$ from \eqref{generative}. 
Here,
$p(\mathbf{f}_d | \mathit{X})$ is a marginal GP prior 
such that 
\begin{align}
p(\bff_d | X) &= \mathcal{N}(\bff_d |\mathbf{0}, K_{NN}) \nonumber \\
                     &= (2 \pi)^{-\frac{N}{2}} \vert K_{NN} \vert^{-\frac{1}{2}} 
                        \exp \left( - \frac{1}{2} \bff_d^\T K_{NN}^{-1} \bff_d \right) , \label{priorF}
\end{align}
where $\mathit{K}_{NN}= \mathit{k}_f(X,X)$ is the covariance matrix
defined by the kernel function $\mathit{k}_f$.

The latent space $X$ plays a significant role in the model since it is
assumed to be the generating space of the observations $Y$. 
Since it is also an unknown random variable, we place a prior distribution $p(X)$ on it, which is
usually selected so as to reflect any known information about the nature of the observed space. Different choices for
this prior lead to models with different properties, as will be discussed in section \ref{}, but for the moment
we will treat $p(X)$ as a general prior and will not consider a special form for it.


If we combine the latent space prior and the GP prior on the mappings with a Gaussian
additive noise term $\epsilon_{nd} \sim \mathcal{N}(0, \beta^{-1})$, the generative process of
equation \eqref{generative} lets us define a joint distribution

\begin{equation}
\label{joint}
p(Y,F,X, \bftheta_f, \beta^{-1}) = p(Y|F, \beta^{-1}) p(F|X, \bftheta_f) p(X)= 
 \prod_{d=1}^D  p(\mathbf{y}_d | \mathbf{f}_d, \beta^{-1}) p(\mathbf{f}_d | X, \bftheta_f) p(X)
\end{equation}
from which we can analytically marginalise out the mappings, even if they are non-linear.
Indeed, the tractable marginal distribution takes the form
%\par Given the latent space prior and the GP prior on the mappings, the latent space $X$ can be optimised so as to
% maximise the likelihood:
\begin{equation}
 \label{gplvmLikelihood}
p(Y | X) p(X) = \int p \left( Y |F\right) p(F | X) p(X) \intd F  ,
\end{equation}
where we have omitted the conditioning on the model hyperparameters $\{ \bftheta_f, \beta \}$ (this simplification will
also be carried on in the rest of the paper).
The above quantity, which constitutes the unnormalised posterior $p(X|Y)$,
can be used as an objective function for obtaining single point estimates for
$X, \bftheta_f$ and $\beta$ (as well as any other parameters associated with $p(X)$).
%Notice that the parameter matrix $W$ does no longer appear in equation \eqref{gplvmLikelihood}, since the prior is placed 
%directly in the function space, something that allows for the consideration of any family of mapping functions. 

\todo[inline]{More about MAP?}



\subsection{Different latent space priors and associated GP-LVM variants \label{section:gplvmDynamics}}

%Alternatively to training the GP-LVM by maximising the likelihood \eqref{gplvmLikelihood}
%with respect to the latent space (and model hyperparameters), 
%one can also incorporate a prior distribution $p(X)$ on the latent variables
%and perform maximum a posteriori (MAP) inference for $X$.  
%The joint distribution of the model now takes the form:
%\begin{equation}
%\label{joint}
%p(Y,F, X) = p(Y|F) p(F|X) p(X)= 
% \prod_{d=1}^D  p(\mathbf{y}_d | \mathbf{f}_d) p(\mathbf{f}_d |
% \mathit{X}) p(X),
%\end{equation}
% \noindent where, again,  we assume independence in the data features given the latent variables.

In the previous section we have presented the common backbone of all GP-LVM variants by
explaining how the joint distribution and the objective unnormalised posterior can be
obtained. In these derivations we have not explicitly defined the for of the prior $p(X)$ on
the latent space. It has already been noted, however, that specific choices for this
prior lead to very different models.

To start with, a standard normal density which factorises across datapoints
constitutes a simple choice for the latent space prior: 
\begin{equation}
\label{standardNormal}
p(X) = \prod_{n=1}^N \mathcal{N}(\bfx_n | \mathbf{0}, I_Q) .
\end{equation}

\noindent This prior does not explicitly model correlations between datapoints and
may only be useful for regularising the optimisation procedure. However,
latent space priors can also be used in order to 
%constrain the latent points as appropriate and, thus, 
incorporate into the model prior information about the data. 
For example, \cite{Urtasun:dgplvm07} add discriminative properties to the GP-LVM by
considering priors which encapsulate class-label information.
Other existing approaches in the literature seek to constrain the latent space via a
smooth dynamical prior $p(X)$ so as to obtain a model for dynamical systems.
For example, \cite{hgplvm} extended GPLVM
with an additional temporal model which employs a GP prior that is able to generate smooth
paths in the latent space.
A similar way of thinking is followed by \cite{GPDM,Wang:gpdm08}, but the prior used
encapsulates the Markov property, resulting in an auto-regressive model. 
\cite{GP-Based,GP-Based2} further extend these models for fully Bayesian filtering
in a robotics setting, whereas \cite{Urtasun:3dpeople06} apply this idea in the context of tracking.

%\subsubsection{Dynamical priors}
%In this paper we focus on the case where the dynamics are regressive.
In this paper we only discuss dynamical extensions where the dynamics are regressive, as in \citep{hgplvm}.
% incorporating dynamics into the standard GP-LVM gives
%a better expressive power to the resulting model, when the application domain is a dynamical system. 
In this setting, a GPLVM-based dynamical model is defined by letting
a temporal latent function $\bfx(t) \in \mathbb{R}^Q$ (with $Q \ll D$) 
govern the intermediate latent space when
generating the data, which are now assumed to be a multivariate
timeseries $\{\bfy_n,t_n\}_{n=1}^N$. Here, $t_n \in \mathbb{R}_+$
refers to the time at which the datapoint $\bfy_n$ was observed. As with
the nonlinear mapping $\bff$, we can consider a Gaussian process prior for
the latent function $\bfx$.
\todo[inline]
{ 
so that its functional form can be
inferred in a fully Bayesian non-parametric fashion without making
strong assumptions. -> this will go later when we motivate our work
}
More formally, each $\bfy_n$ in equation \eqref{generative} is now produced
from $\bfx_n = \bfx(t_n)$, as shown in figure
 \ref{fig:graphicalModels}\subref{fig:dbgplvm}, where:
\begin{equation}
  \label{xt}
  x_q(t)  \sim \mathcal{GP}(0, k_x(t_i,t_j)), \ \ q=1,\ldots,Q .    
\end{equation}

\noindent The individual components of the latent function $\bfx$ are taken to be independent sample paths drawn from
a Gaussian process with covariance function $k_x(t_i,t_j)$. 
Therefore, the marginal GP prior associated with the temporal function $\bfx$ is
\begin{equation}
p(X | \bft)  = \prod_{q=1}^Q p(\bfx_q|\mathbf{t}) = \prod_{q=1}^Q \mathcal{N} \left( \mathbf{x}_q | \mathbf{0},
  \mathit{K_t} \right),
\label{priorXgivenT}
\end{equation}
\noindent where $K_t = k_x(\bft,\bft)$ is the covariance matrix obtained by
evaluating the covariance function $\mathit{k}_x$ on the observed times
$\bft$. In contrast to the trivial prior \eqref{standardNormal}, the above prior couples datapoints across dimensions and, thus,
the correlations between datapoints can be modelled explicitly in the input space.
%
The covariance function $k_x$, \highlight{parametrised by $\bftheta_x$,}
determines the properties of each temporal function $x_q(t)$.
 For instance, the use of an Ornstein-Uhlbeck
covariance function yields a Gauss-Markov process for $x_q(t)$, while
the squared-exponential covariance function gives rise to very smooth and
non-Markovian processes. The specific choices and forms of the covariance
functions used in our experiments are discussed in section \ref{covarianceFunctions}.


\subsection{Drawbacks of the current MAP training procedure \label{mapCriticism}}

Current GP-LVM based models found in the literature rely on MAP training
procedures for optimising the latent variables. 
However, this approach has several drawbacks. Firstly, it 
is prone to overfitting, since the latent points are not associated with
a properly defined posterior distribution. 
Secondly, a MAP training procedure which directly optimises equation
\eqref{gplvmLikelihood} cannot automatically determine an effective dimensionality
for the latent space $X$ in a principled way. In more detail, consider using a GP
covariance function which is has the form of a squared exponential,
like the one in equation \eqref{rbf}, but uses 
a different lengthscale (or weight) per input dimension, as shown below:
\begin{align}
k_{f(ard)}  \left( \mathbf{x}_i, \mathbf{x}_j \right) = {} &  
		\sigma_{ard}^2 \exp \left(
			- \frac{1}{2} \sum_{q=1}^{Q} w_q \left(
                          \mathit{x_{i,q} - x_{j,q}} \right) ^2 \right).
\label{ard}
\end{align}
This covariance function could allow an Automatic Relevance Determination (ARD)
procedure to take place, during which unnecessary dimensions of the latent space
$X$ are assigned an almost weight $w_q$. 
However, in the standard method for training the
GP-LVM, the number of parameters increases proportionally to the number of latent
dimensions, since the training procedure relies on maximising an objective which is a
function of $X$ (among other quantities).
Therefore, the objective function cannot provide any insight for the optimal number of latent dimensions,
since it always increases when more dimensions are added.
This is why the GP-LVM based models found in the literature require that the
dimensionality $Q$ of the latent space is either set by hand or selected after exhaustive model
comparison. However, the later case renders the whole optimization procedure computationally intractable and, in practise,
only a limited subspace of the whole family of models can be explored in a reasonable time.
This constraint severely limits the flexibility of the models and obscures the development
of more sophisticated methods, for example GP-LVM based mixtures where each component
is allowed to have its own complexity. 

\todo[inline]{Mention L1 regularizers for the weights?? Would they ask for comparisons? (is anyone actually using GPLVM+ARD+L1 in the literature?)}

%%This covariance function enables an automatic relevance determination (ARD) procedure and the reason for opting for it
%%will become apparent later in this paper, when the Bayesian training framework for GP-LVM will be described.
%The reason for opting for the above covariance function is that the variational Bayesian training
%framework, presented in the next section, will enable an automatic relevance determination (ARD) procedure
%that automaticaly selects the latent dimensionality.


%However, this introduces several problems that
%severely limit the flexibility, robustness and applicability of the models. Indeed,
%all of these models are prone to overfitting during optimisation and also 
%require the latent space dimensionality to be set by hand a priori or to be
%selected after exhaustive model comparison. This also means
%that increasingly complex models, such as mixtures of GP-LVMs, become unrealistic.


%The variational framework presented in the next section not only enables
%the automatic determination of the model complexity, but also results in a training procedure which is robust 
%to overfitting, since the uncertainty in the latent space is treated in a principled Bayesian manner where the
%latent points are marginalised out. 


On the other hand, being able to marginalise out the latent space would allow for the locale
of the latent points (or the whole latent process for the dynamical extension)
to be inferred in a fully Bayesian non-parametric way. Such an approach not only enables
the automatic determination of the model complexity, but also results in a training procedure which is robust 
to overfitting, since the uncertainty in the latent space is treated in a principled Bayesian manner.

However, attempting to marginalise $X$ from the objective function of the GP-LVM leads to an
intractable integral due to the non-linear way in which $X$ appears in the covariance function $k_f$.
The main contribution of this paper is to provide a framework which allows for approximate
Bayesian inference for GP-LVMs, based on non-standard variational approximations.
%We cannot marginalise both X and the mapping... discussion about marginalisation in pca, gplvm, bayesian pca.
In the following we discuss the basics of approximate variational inference, illustrate how the standard
framework cannot lead to an analytical solution for a Bayesian treatment of GP-LVM and, finally, demonstrate
how such an analytical solution can be obtained by expanding the probability space of the model.


%\par Given the above, we remind the reader that the main focus of this paper is to show how $X$ can also be %marginalised out
%from \eqref{gplvmLikelihood} (a task which is typically intractable), so that
%the optimisation procedure can be formulated in an approximate Bayesian way.





%%----------------------------- SPARSE GPs --------------------------------------------------------------------------------------
%\subsection{\label{sparseGPs} Sparse Gaussian processes}
%
%Probabilistic models based on Gaussian processes are nonparametric and, thus, grow with the number of datapoints. 
%Sparse methods have been developed for that reason, which allow for ...
%
%
%...In our variational framework, inducing inputs are useful not only for reducing the time complexity but also
% because they make the ... tractable...
%
