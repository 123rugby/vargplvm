
%----------------------------------------------- CONCLUSION -----------------------------------------------
\section{\label{section:conclusion} Conclusion}

We have introduced an approximation to the marginal likelihood of the Gaussian process latent variable model
in the form of a variational lower bound. This provides a Bayesian training procedure which is robust
to overfitting and allows for the appropriate dimensionality of the latent space to be automatically determined.
Our framework is extended for the case where the observed data consitute multivariate timeseries and,
therefore, we obtain a very generic method for dynamical systems modelling able to capture complex, non-linear
correlations. We demonstrated the advantages of the rigorous lower bound defined in our framework on a range of
disparate real world data sets. This also emphasised the ability of the model to handle vast dimensionalities.

\par Our approach was easily extended to be applied to training Gaussian processes with uncertain inputs where these
inputs have Gaussian prior densities. For future research, we envisage several other extensions that become computationally
feasible using the same set of methodologies we espouse. Latent space priors other than the
temporal Gaussian process used here, would result in non-linear dynamical models with different properties. In
particular, a Kalman filter or an auto-regressive Gaussian process \cite{GPDM} can be considered.
Further, being able to select the latent space dimensionality automatically makes future hierarchical
or mixture Bayesian modelling approaches seem promising. This is because the key difficulty with the
GP-LVM based methods
considered so far (\eg \cite{hgplvm, Salzmann:2010vh}) is that the dimensionality of the latent
space for each level of the hierarchy or for
each element of the mixture must be set by hand or with ad-hoc techniques, something which is clearly
suboptimal and inefficient.