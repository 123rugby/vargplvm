\documentclass{beamer}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epsfig}

%\usepackage{beamerthemeJuanLesPins}  % quite good
%\usepackage{ beamerthemeMadrid}   % reasonable
%\usepackage{beamerthemeRochester} % very good, rectugularized
%\usepackage{beamerthemeSingapore} % very smooth and gives good space
%\usepackage{beamerthemeWarsaw} % excellent only the bottom problematic 
%\usepackage{}

\include{utils3e}

\title{Bayesian Gaussian Process Latent Variable Model}

\author{{\bf Michalis K. Titsias and  Neil D. Lawrence}\\ 
School of Computer Science, \\
University of Manchester}
\date{}


\begin{document}

\frame{\titlepage}


%\section[Outline]{}
%\frame{\tableofcontents}



\frame
{
\frametitle{Motivation}


\begin{itemize}


\item Gaussian processes are applied to supervized learning


\begin{itemize} 

\item Inputs are fixed/\textcolor{blue}{deterministic}. But what if 
       inputs are \textcolor{blue}{random}?   
       
\end{itemize}

\item \textcolor{blue}{Gaussian process latent variable model} (GP-LVM) is trained  
      by \textcolor{blue}{optimizing} the latent variables 

\begin{itemize}

\item How can we apply a more full \textcolor{blue}{Bayesian} approach where
      latent variables are approximately \textcolor{blue}{marginalized out}? 

\end{itemize}


\item We introduce a \textcolor{red}{variational Bayes} framework:

\begin{itemize}
      
      \item Deal with uncertain/missing inputs in GPs

      \item Automatically select the dimensionality of the non-linear 
            latent space in GP-LVM
 
      \item Learn non-parametric/non-linear 
            dynamical systems (future work)    
 
\end{itemize} 

\end{itemize} 

}




\frame
{
\frametitle{Outline}

\begin{itemize}

\item \textcolor{blue}{Variational inference} for GPs with random (uncertain/missing/latent) inputs 

\begin{itemize}

     \item The role of \textcolor{red}{inducing variables} 

     \item The variational lower bound 
  
    % \item The 

    % \item Prediction 

\end{itemize} 

\item Variational inference for GP-LVM  

\begin{itemize}

  \item \textcolor{blue}{Automatic selection} of the latent
        dimensionality with the squared exponential ARD kernel 

\end{itemize} 

\item Experiments with GP-LVM

\item Summary 
 
\end{itemize} 

}


\frame
{

\frametitle{Gaussian Processes: Deterministic inputs}

\begin{itemize}

\item Gaussian process (GP) is used as non-parametric prior 
      over some latent function \textcolor{red}{$f(\bfx)$}
      %specified 
      %by mean and covaraince/kernel function 
      %$\small \mu(\bfx) = \mathbb{E}f(\bfx)$ and $k(\bfx,\bfx') =
      %\mathbbm{E}(f(\bfx)-\mu(\bfx))(f(\bfx') -\mu(\bfx'))$


\item \textcolor{blue}{Supervised learning}: Estimate
      regression functions, decision boundaries, intensities etc


\end{itemize}


\begin{columns}
\begin{column}[t]{8cm}

\begin{itemize}

\item \textcolor{blue}{Probability model:} Output-input data $(\bfy,X)$: 


\begin{eqnarray}
p(\bfy,\bff|X) & = &  p(\bfy|\bff) \times p(\bff|X) \nonumber \\
\text{\textcolor{red}{Joint}} & = & \text{\textcolor{red}{Likelihood}}
\times \text{\textcolor{red}{marginal GP on X}} \nonumber
\end{eqnarray}
Posterior process %$p(f(\cdot)|\bfy)$
 is Gaussian when $p(\bfy|\bff)$ is Gaussian
 
\end{itemize}

\end{column}

\begin{column}[t]{2cm}
\begin{figure}
%\begin{l}
\includegraphics[width=7mm,height=20mm]{net1.pdf}
%\end{l}
\end{figure}

\end{column}
\end{columns}

\vspace{0.3cm}

%\textcolor{red}
{\bf But what if the inputs $X$ are random (uncertain/missing/latent)}

}



\frame
{

\frametitle{Gaussian Processes: Random inputs}



\begin{columns}
\begin{column}[t]{8cm}

\begin{itemize}
\item \textcolor{blue}{Probability model:} As before, but now the inputs $X$ 
        are given a prior (e.g.\ Gaussian) distribution  $p(X)$: 

$$
p(\bfy,\bff,X) =  p(\bfy|\bff) p(\bff|X) p(X) 
$$


\end{itemize}

\end{column}

\begin{column}[t]{2cm}
\begin{figure}
%\begin{l}
\includegraphics[width=8mm,height=33mm]{net.pdf}
%\end{l}
\end{figure}

\end{column}
\end{columns}

%\vspace{0.3cm}

\begin{itemize}

\item \textcolor{blue}{Random inputs} can be: 


\begin{itemize}

\item \textcolor{blue}{Uncertain inputs}, i.e.\ noisy input measurements

\item \textcolor{blue}{Missing values} in $X$

\item \textcolor{blue}{Latent variables} in non-linear probabilistic PCA 
      (GP-LVM) 


\end{itemize}

\item The \textcolor{red}{posterior} distribution $p(\bff,X|\bfy)$ and the \textcolor{red}{marginal 
      likelihood} $p(\bfy)$ are \textcolor{red}{intractable} 


\end{itemize}


%\textcolor{red}
{\bf Can we apply the standard mean field approxiamtion?}


}



\frame
{

\frametitle{Variational inference: Difficult to apply}


\begin{itemize}

\item Standard regression with random inputs:

$$
p(\bfy,\bff,X) = \underbrace{\mathcal{N}(\bfy|\bff,\sigma^2
  I)}_{\text{Gaussian Lik.}} 
\underbrace{p(\bff|X)}_{\mathcal{N}(\bff|\bfzero, K_{NN})} \underbrace{p(X)}_{\text{Gaussian}} 
$$
%where $p(\bff|X) = \mathcal{N}(\bff|\bfzero, K_{NN})$ and $p(X)$ 
%Gaussian

\item A mean field approximation $q(\bff,X) = q(\bff) q(X)$
      is difficult to apply:

      \begin{itemize}

       \item \textcolor{red}{$X$ appears non-linearly inside the kernel matrix inverse
             $K_{NN}^{-1}$} 
       
       \item Seems impossible to compute the variational bound 
               $\int q(\bff,X) \log \frac{p(\bfy,\bff,X)}{ q(\bff,X)}
               d \bff d X$ 
 

      \end{itemize}       

\item \textcolor{blue}{But there is a trick:} 

      \begin{itemize}

       \item Augment with a finite set of auxiliary parameters 

\begin{itemize}

       \item These will be extra points of the function $f(\bfx)$
         called inducing variables 
           
\end{itemize}
 
      \end{itemize}

\end{itemize}

%{\bf But why we need auxiliary parameters?}

}



\frame
{

\frametitle{Variational inference: Why we need auxiliary parameters?}

\begin{itemize} 

\item Bayesian \textcolor{blue}{linear regression} with random inputs  

      $$
      \bfy = X \bfw + \bfepsilon, \ \  p(\bfw) = N(\bfzero, \sigma_w^2
      I)
      $$
     
     \begin{itemize}
      \item It is straightforward to apply mean field 
            using $q(\bfw)q(X)$
     \end{itemize}
      
\item \textcolor{blue}{Kernelization:} We can express the model in
     a function (non-parametric) view as

       
      $$
      \bfy = \bff + \bfepsilon, \ \  p(\bff) = N(\bfzero, \sigma_w^2 XX^T)
      $$
     
      where the  GP prior has a linear kernel  

\item The kernelization makes variational inference  difficult % on $X$ intractable 

       \begin{itemize}
            \item \textcolor{red}{$X$ appears in the inverse of $X X^T$} 
               %cannot be integrated out (neither exactly nor
               %variationally)

             \item Not clear how to apply mean field using 
                   $q(\bff) q(X)$

       \end{itemize}


%  \item But if we keep $\bfw$, then
%    variational inference %in the space of $(\bfw,X)$
% is tractable  

\end{itemize}


}





\frame
{

\frametitle{Variational inference: Why we need auxiliary parameters?}


\begin{itemize} 


\item Gaussian processes (kernel methods in general) are somehow 
\textcolor{blue}{marginalized (collapsed)} 
      

\begin{itemize} 

\item A GP is an \textcolor{blue}{exchangeable} model:

 $$
 p(f_1,\ldots,f_N) = \int \prod_{n=1}^N p(f_n|\bfw) d P(\bfw)
 $$
 
 where the underlying (infinite) parameter $\bfw$ has been integrated out 

\item We are left with the kernel function and \textcolor{blue}{inputs that 
      appear inside matrix inverses} 
  

\end{itemize}
 

\item We need to discover some (approximate) parameters to apply 
      variational inference 

%\item These parameter will be auxiliary function points 
%      called inducing variables 

\end{itemize}

{\bf The parameters we use are auxiliary function points used in sparse
  GPs, called inducing variables}

}





\frame
{

\frametitle{Inducing variables: The general idea} 

\begin{columns}
\begin{column}[t]{8cm}


\begin{itemize}
\item Initially
$$
p(\bfy,\bff,X) = \mathcal{N}(\bfy|\bff,\sigma^2
  I) p(\bff|X) p(X) 
$$
(\textcolor{blue}{variational inference in the space of $(\bff,X)$ is difficult})

\end{itemize}



\end{column}

\begin{column}[t]{3cm}
\begin{figure}
%%\begin{l}
\includegraphics[width=8mm,height=30mm]{net.pdf}
%\end{l}
\end{figure}
\end{column}
\end{columns}


\begin{columns}
\begin{column}[t]{8cm}


\begin{itemize}

\item Augment \textcolor{red}{constistently}\footnote{$\int
    p(\bff|\bfu, X) p(\bfu) d \bfu =
    p(\bff|X)$}
 with inducing variables 

\begin{eqnarray}
p(\bfy,\bff,\bfu, X) 
%& = & 
%\mathcal{N}(\bfy|\bff,\sigma^2I)
%\textcolor{red}{p(\bff,\bfu| X)} p(X) \nonumber \\
& = & \mathcal{N}(\bfy|\bff,\sigma^2I)
\textcolor{red}{p(\bff | \bfu, X) p(\bfu)} p(X) \nonumber
\end{eqnarray}

(\textcolor{blue}{variational inference in the space of $(\bff, \bfu, X)$ is tractable})

\end{itemize}

\end{column}

\begin{column}[t]{3cm}
\begin{figure}
%%\begin{l}
\includegraphics[width=20mm,height=30mm]{net2.pdf}
%\end{l}
\end{figure}
\end{column}
\end{columns}

}





\frame
{

\frametitle{Inducing variables: Linear GP} 


%Inducing variables can replace the standard parameter $\bfw$ 
%in PCA  and allow for variational inference in the kernelized model 

\begin{figure}
%\begin{l}
\includegraphics[width=55mm,height=50mm]{Fig11.pdf}
%\end{l}
\end{figure}

\begin{itemize}

\item Illustration of augmented GP with inducing variables:

$$ 
p(\bfy,\bff,\bfu, X) = \mathcal{N}(\bfy|\bff,\sigma^2I)
p(\bff | \bfu, X) p(\bfu) p(X)
$$

\end{itemize}


} 



\frame
{

\frametitle{Inducing variables: Linear GP} 

%Inducing variables can replace the standard parameter $\bfw$ 
%in PCA  and allow for variational inference in the kernelized model 


\begin{figure}
%\begin{l}
\includegraphics[width=55mm,height=50mm]{Fig2.pdf}
%\end{l}
\end{figure}


\begin{itemize}

\item Draw input data $X$:

$$ 
p(\bfy,\bff,\bfu, X) = \mathcal{N}(\bfy|\bff,\sigma^2I)
p(\bff | \bfu, X) p(\bfu) \textcolor{blue}{p(X)}
$$

\end{itemize}


} 


\frame
{

\frametitle{Inducing variables: Linear GP} 

%Inducing variables can replace the standard parameter $\bfw$ 
%in PCA  and allow for variational inference in the kernelized model 

\begin{figure}
%\begin{l}
\includegraphics[width=55mm,height=50mm]{Fig3.pdf}
%\end{l}
\end{figure}

\begin{itemize}

\item Choose some (arbitrary) pseudo-inputs $Z$ 

$$ 
p(\bfy,\bff,\bfu, X) = \mathcal{N}(\bfy|\bff,\sigma^2I)
p(\bff | \bfu, X) p(\bfu) p(X)
$$

\end{itemize}

\textcolor{red}{\bf Crucial: $Z$ are not random variables}


} 


\frame
{

\frametitle{Inducing variables: Linear GP} 

%Inducing variables can replace the standard parameter $\bfw$ 
%in PCA  and allow for variational inference in the kernelized model 

\begin{figure}
%\begin{l}
\includegraphics[width=55mm,height=50mm]{Fig4.pdf}
%\end{l}
\end{figure}

\begin{itemize}

\item Sample random function values $\bfu$ at the pseudo-inputs $Z$ 

$$ 
p(\bfy,\bff,\bfu, X) = \mathcal{N}(\bfy|\bff,\sigma^2I)
p(\bff | \bfu, X)  \textcolor{blue}{p(\bfu)} p(X)
$$

where $p(\bfu) = \mathcal{N}(\bfu|\bfzero, Z Z^T)$ 

\end{itemize}

}


\frame
{

\frametitle{Inducing variables: Linear GP} 

%Inducing variables can replace the standard parameter $\bfw$ 
%in PCA  and allow for variational inference in the kernelized model 

\begin{figure}
%\begin{l}
\includegraphics[width=55mm,height=50mm]{Fig5.pdf}
%\end{l}
\end{figure}

\begin{itemize}

\item Sample function values $\bff$ on training inputs $X$
      so that the function passes from the inducing variables 
             
$$ 
p(\bfy,\bff,\bfu, X) = \mathcal{N}(\bfy|\bff,\sigma^2I)
 \textcolor{blue}{p(\bff | \bfu, X)} p(\bfu) p(X)
$$
 
where $p(\bff | \bfu, X)$ is the conditional GP prior 
(\textcolor{red}{this is the delta function in the example!})

\end{itemize}


}


\frame
{

\frametitle{Inducing variables: Linear GP} 

%Inducing variables can replace the standard parameter $\bfw$ 
%in PCA  and allow for variational inference in the kernelized model 

\begin{figure}
%\begin{l}
\includegraphics[width=55mm,height=50mm]{Fig6.pdf}
%\end{l}
\end{figure}

\begin{itemize}

\item Generate the observed data $\bfy$             

$$ 
p(\bfy,\bff,\bfu, X) = \textcolor{blue}{\mathcal{N}(\bfy|\bff,\sigma^2I)}
   p(\bff | \bfu, X) p(\bfu) p(X)
$$
 
\end{itemize}


}



\frame
{

\frametitle{Variational inference}


\begin{itemize}

%\item True posterior distribution: $
%p(\bff, \bfu, X) = p(\bff|\bfu, \bfy, X) p(\bfu,X|\bfy)
%$

\item Variational distribution: 
$q(\bff, \bfu, X) = p(\bff|\bfu, X) \phi(\bfu) q(X)$

\begin{itemize}

\item $q(X)=\mathcal{N}(\bfmu,\Sigma)$: Gaussian distribution

\item $\phi(\bfu)$: unrestricted (turns out to be Gaussian) 

\item $p(\bff|\bfu, X)$: conditional  GP prior (\textcolor{magenta}{\bf !!trick!!})

\item It is mean field  w.r.t.\ $\bfu$ and $X$

\end{itemize}


\item Maximize the lower bound


\begin{multline}
 \log \int \mathcal{N}(\bfy|\bff,\sigma^2 I) 
p(\bff|\bfu, X) p(X) d \bff d \bfu \bfX \geq \\
\int p(\bff|\bfu, X) \phi(\bfu) q(X)  
\log \frac{\mathcal{N}(\bfy|\bff,\sigma^2 I) 
\textcolor{red}{p(\bff|\bfu, X)} p(\bfu) p(X)}
{ \textcolor{red}{p(\bff|\bfu, X)} \phi(\bfu) q(X)} d \bff d \bfu \bfX
\nonumber 
\end{multline}
%or
%$$
%= \int p(\bff|\bfu, X) \phi(\bfu) q(X)  
%\log \frac{\mathcal{N}(\bfy|\bff,\sigma^2 I) p(\bfu) p(X)}
%{\phi(\bfu) q(X)} d \bff d \bfu \bfX
%$$

\end{itemize}

\textcolor{blue}{\bf This is now tractable. Matrix inverses containing $X$ are gone}


}






\frame
{

\frametitle{Variational inference}

$$
\int p(\bff|\bfu, X) \phi(\bfu) q(X)  
\log \frac{\mathcal{N}(\bfy|\bff,\sigma^2 I) p(\bfu) p(X)}
{\phi(\bfu) q(X)} d \bff d \bfu \bfX
$$


\begin{itemize}

\item Example: in the linear GP (previous figures), 
      $p(\bff|\bfu, X) = \delta(\bff - X A \bfu)$ 
      for some matrix $A$. Then 


$$
\int \phi(\bfu) q(X)  
\log \frac{\mathcal{N}(\bfy|X A \bfu,\sigma^2 I) p(\bfu) p(X)}
{\phi(\bfu) q(X)} d \bfu d \bfX
$$

\item This is the same as mean field in the usual parameters
      $(\bfw,X)$ ( \textcolor{blue}{$\bfu$ is just a linear trasformation of $\bfw$}) 

\end{itemize}


{\bf The methodology is general: i) works not only for the
  linear kernel but for others (e.g.\ squared exponentials) and ii) 
$p(\bff|\bfu, X)$ doesn't have to be a delta function}

}










%\frame
%{
%
%\frametitle{Variational inference}
%
%\begin{itemize}
%
%\item Marginal likelhihood 
%
%$$
%p(\bfy) = \int \mathcal{N}(\bfy|\bff,\sigma^2 I) 
%p(\bff|\bfu, X) p(X) d \bff d \bfu \bfX  
%$$
%%recall that is invariant to the value of $Z$ 
%%
%%\item Variational distrbution:
%% 
%%$$
%q(\bff, \bfu, X) = p(\bff|\bfu, X) \phi(\bfu) q(X)
%$$
%where 
%
%\begin{itemize}
%
%\item $q(X)=\mathcal{\bfmu,\Sigma}$: Gaussian distribution
%
%\item $\phi(\bfu)$: unrestricted (will turn out to be Gaussian) 
%
%\item $p(\bff|\bfu, X)$: conditonal  GP prior that appears in the joint
%
%\end{itemize}
%     
%
%\end{itemize}
%
%\textcolor{red}{The inputs $Z$ are variational parameters, 
%e.g.\ affect the form of the variational distribution through $p(\bff|\bfu, X)$}
%
%}
%
%
%
%\frame
%{
%
%\frametitle{Variational lower bound}
%
%\begin{itemize}
%
%\item Minimization of the $KL[q(\bff,\bfu,X)||p(\bff,\bfu,X|\bfy)]$ is
%  equivalent to the maximization of a Jensen's lower bound
%
%\end{itemize}
%
%\begin{multline}
%\log p(\bfy) \geq F(q) = \nonumber \\
%\int p(\bff|\bfu, X) \phi(\bfu) q(X)  
%\log \frac{\mathcal{N}(\bfy|\bff,\sigma^2 I) 
%\textcolor{red}{p(\bff|\bfu, X)} p(\bfu) p(X)}
%{ \textcolor{red}{p(\bff|\bfu, X)} \phi(\bfu) q(X)} d \bff d \bfu \bfX  
%\end{multline}
%
%\begin{itemize}
%
%\end{itemize}
%
%\item 
%
%where $p(\bff|\bfu, X)$s in the $\log$ cancel out and things simplify: 
%
%
%
%
%\end{itemize}
%
%\begin{align*} 
%& F(q)  \geq \\
%& \int \phi(\bfu)  \left[
% \langle \log \mathcal{N}(\bfy| \bfalpha, \sigma^2 I) \rangle_{q(X)} + 
%  \log \frac{p(\bfu)}{\phi(\bfu)} 
%\right] d \bfu \\ 
%& -  
% \frac{1}{2 \sigma^2} \text{tr} \left( \langle K_{NN} \rangle_{q(X)} \right)
%+ \frac{1}{2\sigma^2} \text{Tr} \left( K_{MM}^{-1} \langle K_{MN}
%K_{NM} \rangle_{q(X)} \right),    
%\end{align*}
%}


%\frame
%{
%
%\frametitle{Variational lower bound}
%
%
%\begin{align*} 
%& F(q)  \geq  \int \phi(\bfu)  \left[
% \langle \log \mathcal{N}(\bfy| \bfalpha, \sigma^2 I) \rangle_{q(X)} + 
%  \log \frac{p(\bfu)}{\phi(\bfu)} 
%\right] d \bfu \\ 
%& -  
% \frac{1}{2 \sigma^2} \text{tr} \left( \langle K_{NN} \rangle_{q(X)} \right)
%+ \frac{1}{2\sigma^2} \text{Tr} \left( K_{MM}^{-1} \langle K_{MN}
%K_{NM} \rangle_{q(X)} \right),    
%\end{align*}
%
%where $\langle \cdot \rangle_{q(X)}$ expectation under  $q(X)$
%
%\begin{itemize}
%
%\item Define the $\Psi$ statistics 
%
%\begin{itemize}
%
%\item $\psi_0 = \text{tr} \left( \langle K_{NN} 
%\rangle_{q(X)} \right)$ (variacne term)
%
%\item $\Psi_1 = \langle K_{NM} \rangle_{q(X)}$ (mean term)
%
%\item $\Psi_2 = \langle K_{MN} K_{NM} \rangle_{q(X)}$
%  (covariacne/interaction term)
%
%\item invlove convolution of the Gaussian $q(X)$ with the kernel
%
%\item Closed-forms for squared exponential and linear kernels 
%
%\end{itemize}
%  
%
%\end{itemize} 
%
%}





%\frame
%{
%
%\frametitle{Variational lower bound}
%
%\begin{itemize}
%
%\item Analytically maximize the bound w.r.t.\ $\phi(\bfu)$: 
%
%\end{itemize} 
%
%$$
%\widetilde{F}(q) = \log \left( \int e^{ \langle \log \mathcal{N}(\bfy| \bfalpha,
%\sigma^2 I) \rangle_{q(X)}} p(\bfu) d \bfu \right)
%- \frac{\psi_0}{2\sigma^2}
%+ \frac{1}{2\sigma^2}  \text{tr} \left( K_{MM}^{-1} \Psi_2 \right) 
%$$
%
%\begin{itemize} 
%
%\item Final form 
%
%\end{itemize}
%
%\begin{multline}
%F(q) =   \log \frac{1}{(2 \pi \sigma^2)^{\frac{N-M}{2}})}\frac{|K_{MM}|^{\frac{1}{2}}}
%{|\Psi_2 + \sigma^2 K_{MM}|^{\frac{1}{2}}} 
%- \frac{1}{2 \sigma^2} \bfy^T \bfy \nonumber \\
%+  \frac{1}{2 \sigma^2} \bfy^T \Psi_1 (
%\Psi_2 + \sigma^2 K_{MM})^{-1} \Psi_1^T \bfy 
% -  \frac{\psi_0}{2 \sigma^2}  +  
%\frac{1}{2\sigma^2} \text{tr} \left(K_{MM}^{-1} \Psi_2 \right),
%\end{multline}
%
%\textcolor{red}{Same form with the stardard sparse
%GP variational bound (Titsias, 2009). Now 
%the kernel quantities containing $X$ have been replaced 
%by variational averages}
%
%}
%
%
%\frame
%{
%
%\frametitle{Variational lower bound}
%
%\begin{multline}
%F(q) =   \log \frac{1}{(2 \pi \sigma^2)^{\frac{N-M}{2}})}\frac{|K_{MM}|^{\frac{1}{2}}}
%{|\Psi_2 + \sigma^2 K_{MM}|^{\frac{1}{2}}} 
%- \frac{1}{2 \sigma^2} \bfy^T \bfy \nonumber \\
%+  \frac{1}{2 \sigma^2} \bfy^T \Psi_1 (
%\Psi_2 + \sigma^2 K_{MM})^{-1} \Psi_1^T \bfy 
% -  \frac{\psi_0}{2 \sigma^2}  +  
%\frac{1}{2\sigma^2} \text{tr} \left(K_{MM}^{-1} \Psi_2 \right),
%\end{multline}
%
%It is maximzied w.r.t.\ 
%
%\begin{itemize}
%
%\item Inducing inputs $Z$ (variational parameters) 
%
%\item Variational dist. q(X), i.e.\ $\bfmu$ and $\Sigma$
%     (variational parameters)
%
%\item Model hyperpramaters, i.e.\ the parameters of the kernel
%  fucntion and noise variance $\sigma^2$   
%
%
%\item Gradient-based optimization is used, e.g.\ scaled conjugate gradients
%
%
%\end{itemize}
%
%}



\frame
{
\frametitle{Gaussian process latent variables model (Lawrence, 2005)}


\begin{columns}
\begin{column}[t]{8cm}

\begin{itemize}
\item \textcolor{blue}{Latent variable model:} 

$$
\bfy  = \bff(\bfx)  + \bfepsilon 
$$

\begin{itemize}

\item $\bfy \in \mathbbm{R}^D$: observed variable
\item $\bfx \in \mathbbm{R}^Q$ ($Q \ll D$): latent variable 
\item $\bff : \mathbbm{R}^Q \rightarrow \mathbbm{R}^D$: latent mapping 

\item GP-LVM: GP priors on the latent mapping

\end{itemize}
\end{itemize}

\end{column}

\begin{column}[t]{4cm}
\begin{figure}
%\begin{l}
\includegraphics[width=25mm,height=33mm]{Figure1.pdf}
%\end{l}
\end{figure}

\end{column}
\end{columns}

\vspace{0.3cm}

GP-LVM is trained by \textcolor{blue}{optimizing} 
      (\textcolor{blue}{not marginalizing} out) the latent variables

\begin{itemize}

\item Not proper density in the latent space 

\item Cannot select the latent dimensionality $Q$ 

\item It may overfit since it is not fully Bayesian 

\end{itemize}


}



\frame
{
\frametitle{Bayesian Gaussian process latent variables model}


\begin{columns}
\begin{column}[t]{8cm}

\begin{itemize}
\item Latent variable model: 

$$
\bfy  = \bff(\bfx)  + \bfepsilon 
$$

\item \textcolor{blue}{Bayesian training:} Integrate out both the latent mapping 
      and the latent space  

\begin{itemize}

\item Exact Bayesian inference is intractable 

\item But variational Bayesian inference is tractable 
 

\end{itemize}
\end{itemize}

\end{column}

\begin{column}[t]{4cm}
\begin{figure}
%\begin{l}
\includegraphics[width=25mm,height=33mm]{Figure1.pdf}
%\end{l}
\end{figure}

\end{column}
\end{columns}

\vspace{0.5cm}
 

{\bf The variational method is applied as before. The only 
      difference is that now we have $D$ latent functions 
      (one for each observed output)
 and not just one}  

}




\frame
{
\frametitle{Bayesian Gaussian process latent variables model}


\textcolor{blue}{Automatic selection of the latent dimensionality}

\begin{itemize} 



\item Squared exponential ARD kernel

$$
k(\bfx,\bfx') = \sigma_f^2 \exp\left( - \frac{1}{2} \sum_{q=1}^Q
\alpha_q (x_q - x_q')^2 \right)
$$


\item Maxiziming the variational lower bound w.r.t.\ 
      $\alpha_q$s allows to remove  
      redundant latent dimensions   

\end{itemize}

}


\frame
{
\frametitle{Experiments: Visualization}


\begin{itemize}


\item  Oil flow data: $1000$ training; $12$ dimensions; 3 known classes


\item Compare: 


 \begin{itemize} 

 \item Bayesian GP-LVM

\item Standard sparse GP-LVM 

\item Probabilistic PCA 
 
 \end{itemize}

 \end{itemize}

}


\frame
{
\frametitle{Experiments: Visualization}

Oil flow data: 

%\begin{figure*}[ht]
\begin{center}
\begin{tabular}{cc}
\includegraphics[width=42mm,height=35.5mm]
{demOilVargplvmLengthScales1.pdf}&
\includegraphics[width=42mm,height=35.5mm]
{demOilVargplvm1.pdf}\\
($\alpha_q$s) & (Bayesian GP-LVM) 
\end{tabular}
%\caption{Panel (a) shows the inverse lengthscales found by applying the
%  Bayesian GP-LVM with ARD SE kernel on the oil flow data. Panel (b)
%  shows the visualization achieved by keeping the most dominant latent
%  dimensions (2 and 3) which have the largest inverse lengthscale
%  value. Dimension 2 is plotted on the
%  $y$-axis and 3 and on the $x$-axis. Plot (c) shows the visualization found
%  by standard sparse GP-LVM.
%\label{fig:Oil}}
\end{center}
%\end{figure*}

\begin{itemize}


\item Bayesian GP-LVM runs with $10$ latent dimensions

\item The \textcolor{red}{red}, \textcolor{green}{green} and 
     \textcolor{blue}{blue} points are the predicted
     means for the latent variables labeled with the known class 

\item \textcolor{red}{$7$ out $10$ latent are shrunk to zero} 

\item Visualization is shown for the dominant (with the largest
  inverse lengthscales) latent dimensions 

\end{itemize}   

}


\frame
{
\frametitle{Experiments: Visualization}

Oil flow data

%\begin{figure*}[ht]
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=34mm,height=30.5mm]
{demOilVargplvm1.pdf} &
\includegraphics[width=34mm,height=30.5mm]
{demOilFgplvm7.pdf} &
\includegraphics[width=34mm,height=30.5mm]
{pcaOil.pdf}\\
(Bayesian GP-LVM) & (GP-LVM) & (PPCA)
\end{tabular}
%\caption{Panel (a) shows the inverse lengthscales found by applying the
%  Bayesian GP-LVM with ARD SE kernel on the oil flow data. Panel (b)
%  shows the visualization achieved by keeping the most dominant latent
%  dimensions (2 and 3) which have the largest inverse lengthscale
%  value. Dimension 2 is plotted on the
%  $y$-axis and 3 and on the $x$-axis. Plot (c) shows the visualization found
%  by standard sparse GP-LVM.
%\label{fig:Oil}}
\end{center}
%\end{figure*}


GP-LVM and Bayesian GP-LVM are both initialized based on PCA


}


%\frame
%{
%\frametitle{Experimetns with Bayesian GP-LVM}
%
%Inducing inputs $Z$ (pink diamonds)
%
%%\begin{figure}[ht]
%\begin{center}
%\begin{tabular}{c}
%\includegraphics[width=50mm,height=40mm]
%{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/temp/demOilVargplvm1WithInduc.pdf}\\
%(inverse lengthscales: $\alpha_q$s)
%%\caption{This plot shows the values of the inverse lengthscales
%%found by using the Bayesian GP-LVM with ARD SE kernel in Frey faces.  
%%\label{fig:BrendanInverseLenghScale}}
%\end{tabular}
%\end{center}
%%\end{figure}
%
%
%\begin{itemize}
%
%\item Inducing inputs are forced to be close to the means of the
%  variational distribution $q(X)$, i.e.\ close to the GP inputs
%
%\item This is consequence of the variarional method 
%      for standard sparse GPs (Titsias, 2009)
%
%\end{itemize}   
%
%}


\frame
{
\frametitle{Experiments: Predict missing values}

Frey faces: $1965$ images; $28 \times 20=560$ dimensions;
 $1000$ for training; $965$ for testing 

%\begin{figure*}[ht]
\begin{center}
\begin{tabular}{ccccccc}
\small{True}& \includegraphics[width=12mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag1_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag2_3.pdf} &
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag4_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag11_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag24_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag51_3.pdf}\\
\small{Test} &
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag1WithMissing_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag2WithMissing_3.pdf} &
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag4WithMissing_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag11WithMissing_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag24WithMissing_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag51WithMissing_3.pdf}\\
\small{Recon.} &
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag1Reconst_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag2Reconst_3.pdf} &
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag4Reconst_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag11Reconst_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag24Reconst_3.pdf}&
\includegraphics[width=12mm,height=12mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag51Reconst_3.pdf}
\end{tabular}
%\caption{Examples of reconstruction of partially observed test images 
%in Frey faces by applying the Bayesian GP-LVM. Each column corresponds to a test
%image. In every column, the top panel shows the true test image, 
%the middle panel the partially observed image (where missing pixels are
%shown in black) and the bottom image is the reconstructed image. 
%\label{fig:BrendanFaces}}
\end{center}
%\end{figure*}

\begin{itemize}

%\item Train with $1000$ images; reconstuct the missing pixels ($50\%$)
%  in the remaining $965$ test images 
%
%\item Row 1: original test images. Row 2: image with missing
%  pixels given to the algorithm. Row 3: reconstruct images


\item Bayesian GP-LVM is trained with $30$ latent dimensions 
     \textcolor{blue}{mean absolute reconstruction error: $7.4003$}


\item Standard sparse GP-LVM is trained with several latent dimensions:
      $Q= 2, 5, 10, 30$. Errors: \textcolor{blue}{$10.5748, 9.7284, 19.6949, 19.6961$}


\end{itemize}   

}

\frame
{
\frametitle{Experiments: Generative classification}

\begin{itemize} 

\item  \textcolor{blue}{USPS digits dataset:} $16 \times 16$ images for all $10$ digits,  $7291$
training examples and $2007$ test examples

\item Run $10$ Bayesian GP-LVMs: one for each digit

\item Compute Bayesian class conditional densities in the test data of
  the form $p(\bfy_*|Y,\text{digit})$

\end{itemize}

Results: \textcolor{blue}{From $2007$ test images we have $95$ incorrectly
classified digits, i.e.\  4.73\% error}

}



%\frame
%{
%\frametitle{Experimetns with Bayesian GP-LVM}
%
%Fray faces: $1965$ images, $28 \times 20 =560$ dimensions
%
%%\begin{figure}[ht]
%\begin{center}
%\begin{tabular}{c}
%\includegraphics[width=50mm,height=40mm]
%{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanVargplvmLengthScales3.pdf}
%%\caption{This plot shows the values of the inverse lengthscales
%%found by using the Bayesian GP-LVM with ARD SE kernel in Frey faces.  
%%\label{fig:BrendanInverseLenghScale}}
%\end{tabular}
%\end{center}
%%\end{figure}
%
%
%
%\begin{itemize}
%
%\item Bayesian GP-LVM trained with 30 latent dimensions 
%     mean absolute reconstruction error: $7.4003$
%
%
%\item Standard GP-LVM trained with several latent dimensions:
%      $Q= 2, 5, 10, 30$. Errors: $10.5748, 9.7284, 19.6949, 19.6961$
%
%\end{itemize}   
%
%
%
%}



\frame
{
\frametitle{Summary/Future work}


Summary: 

\begin{itemize}

\item A variational framework to approximately integrate 
      out inputs in GPs 

\item Allows for Bayesian training procedure of GP-LVM
        
\end{itemize} 

Future work: 

\begin{itemize}

\item Optimization: Currently we use conjugate gradients
           to jointly maximize the lower bound over variational 
           and model parameters

\begin{itemize} 

\item Improvements: Find fixed point updates, 
      explore the correlation structure of the optimized parameters

 \end{itemize}

 \item Learn non-parametric/non-linear 
         dynamical systems using GPs and variational Bayes    

\end{itemize}

}




\end{document}