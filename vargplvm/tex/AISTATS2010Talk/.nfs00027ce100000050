\documentclass{beamer}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{bbm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{epsfig}

\usepackage{beamerthemeJuanLesPins}  % quite good
%\usepackage{ beamerthemeMadrid}   % reasonable
%\usepackage{beamerthemeRochester} % very good, rectugularized
%\usepackage{beamerthemeSingapore} % very smooth and gives good space
%\usepackage{beamerthemeWarsaw} % excellent only the bottom problematic 
%\usepackage{}

\include{utils3e}

\title{Bayesian Gaussian Process Latent Variable Model}

\author{{\bf Michalis K. Titsias and  Neil D. Lawrence}\\ 
School of Computer \\
Science University of Manchester}
\date{}


\begin{document}

\frame{\titlepage}


%\section[Outline]{}
%\frame{\tableofcontents}



\frame
{
\frametitle{Motivation}


\begin{itemize}

\item Train Gaussian process (GP)  models
      when inputs are random variables   

\item Train the Gaussian process latent variable model 
      (GP-LVM) 
      by marginalizing out (not optimizing) the latent variables 

\item Can we apply variational Bayes?



\item Possible benefits:

\begin{itemize}
      
      \item Deal with missing inputs in GPs

      \item Automatic selection of the dimensionality of the non-linear 
            manifold in GP-LVM 
 
      \item (Future work) Bayesian learning of fully non-parametric 
            dynamical systems    
 
\end{itemize} 

\end{itemize} 

}




\frame
{
\frametitle{Outline}

\begin{itemize}

\item Variational inference for GPs with random inputs 

\begin{itemize}

     \item Tnducing variables  

     \item The variational lower bound 
  
    % \item The 

    % \item Prediction 

\end{itemize} 

\item Variational inference for GP-LVM  

\begin{itemize}

  \item Model selection with squared exponential ARD kernel 

\end{itemize} 

\item Experiments with GP-LVM

\item Summary 
 
\end{itemize} 

}


\frame
{

\frametitle{Gaussian Processes: Fixed inputs}

\begin{itemize}

\item Gaussian process (GP) are used as non-parametric priors 
       over latent function \textcolor{red}{$f(\bfx)$}, specified 
      by mean and covaraicne/kernel function 
      %$\small \mu(\bfx) = \mathbb{E}f(\bfx)$ and $k(\bfx,\bfx') =
      %\mathbbm{E}(f(\bfx)-\mu(\bfx))(f(\bfx') -\mu(\bfx'))$


\item Supervised learning: 
      regresion functions, decision boundaries, intensities,
      protein concentrations etc


\item Probability setting: Output-input data $(\bfy,X)$: 


\begin{eqnarray}
p(\bfy,\bff|X) & = &  p(\bfy|\bff) \times p(\bff|X) \nonumber \\
\text{\textcolor{red}{Joint}} & = & \text{\textcolor{red}{Likelihood}}
\times \text{\textcolor{red}{marginal GP on X}} \nonumber
\end{eqnarray}
Posterior process %$p(f(\cdot)|\bfy)$
 is Gaussian when $p(\bfy|\bff)$ is Gaussian
 
\end{itemize}

\textcolor{red}{Question: what if inputs $X$ are random (missing/latent) 
and need to be marginalized out?}

}



\frame
{

\frametitle{Gaussian Processes: Random inputs}


\begin{itemize}

\item  Probability model: As before, but the inputs $X$ 
       (or some of them) are given a prior (Gaussian) distirbution 
       $p(X)$: 

$$
p(\bfy,\bff,X) =  p(\bfy|\bff) p(\bff|X) p(X) 
$$

\item Random inputs can be: 


\begin{itemize}

\item Uncertain inputs: noisy inputs measurements

\item Missing values in $X$

\item Latent variables in non-linear probablsitic PCA 
      (Gaussian process latent variable model) 


\end{itemize}


\item The posterior distribution $p(\bff,X|\bfy)$ and the marginal 
      likelihood  $p(\bfy)$ are intractable 


\end{itemize}


\textcolor{red}{We will introduce a variational Bayes framework}


}



\frame
{

\frametitle{Variational inference}

\begin{itemize}

\item Standard regression with random inputs:

$$
p(\bfy,\bff,X) = \mathcal{N}(\bfy|\bff,\sigma^2 I) p(\bff|X) p(X) 
$$
where $p(\bff|X) = \mathcal{N}(\bff|\bfzero, K_{NN})$ and $p(X)$ 
Gaussian

\item A mean field $q(\bff,X) = q(\bff) q(X)$
      is intctable:

      \begin{itemize}

       \item $X$ appears no-linearly inside the inverse
             $K_{NN}^{-1}$ 
       
       \item It helpless to try to compute a lower bound 
 

      \end{itemize}       

\item But there is a trick: 

      \begin{itemize}

       \item Use data augmentation and introduce a finite set of parameters 


       \item These will be extra points of the function $f(\bfx)$
         called inducing variables 
           
 
      \end{itemize}

\end{itemize}

}



\frame
{

\frametitle{Variational inference: why we need auxiliary parameters?}

An analogy with linear parametric models 

\begin{itemize} 

\item Linear regression model with random inputs 
      (i.e.\ PCA)       

      $$
      \bfy = X \bfw + \bfepsilon, \ \  \text{with s.n.\ priors} \  p(\bfw),
      p(X)
      $$
     

\item When $\bfw$ is marginalized the model is kernelized. It is a GP 
      
       $$
        p(\bfy,X) = \mathcal{N}(\bfy|\bfzero, I + X X^T) p(X)
       $$

      %having the linear kernel $K_{NN} = X X^T$ plus white 

\item The kernelization (collapsing the model) makes inference on $X$ intractable 

       \begin{itemize}
            \item $X$ appears in the inverse of $(I + X X^T )$, 
               cannot be integrated out (neither exactly nor variationally)

       \end{itemize}


  \item But if we keep $\bfw$, then
    variational inference %in the space of $(\bfw,X)$
 is tractable  

\end{itemize}

}




\frame
{

\frametitle{Variational inference: why we need auxiliary parameters?}


\begin{itemize} 


\item Gaussian processes (kernel methods in general) are somehow 
martginalized (collapsed) 
      

\begin{itemize} 

\item We are left with a kernel function $k(\bfx,\bfx')$ and
  exchangeable model:

 $$
 p(f_1,\ldots,f_N) = \int \prod_{n=1}^N p(f_n|\bfw) d P(\bfw), \ \ \text{de Finetti}
 $$
 
\item The underlying finite or infinite parameter 
      $\bfw$ has been integrated out 
 

\end{itemize}
 

\item We need to put back (discover) approximate parameters to apply 
      variational inference 

\item These parameter will be auxiliary function points 
      called inducing variables 

\end{itemize}


}






\frame
{

\frametitle{Variational inference}



\begin{itemize}


\item Add extra function points (called inducing variables)  
      $\bfu \in \mathbbm{R}^{M}$ into the GP prior 
      evaluated at inducing inputs $Z$   

$$ p(\bfy,\bff,\bfu, X) =  p(\bfy|\bff)
p(\bff | \bfu, X) p(\bfu) p(X)
$$
where 
$$
p(\bff|\bfu, X) = \text{conditional GP prior}
$$

$$ 
p(\bfu) = \text{marginal GP on Z}
$$

\item The inducing inputs $Z$:

      \begin{itemize} 

       \item Do not affect the exact model, marginal likelihood $p(y)$ and  posterior 
             $p(\bff,X|\bfy)$ are invariant to $Z$
              
       \item Are not random variables, neither model hyperparameters. 
            We will turn them into variational parameter  

      \end{itemize}


\end{itemize}

}




\frame
{

\frametitle{Variational inference}

Illustration of inducing variables

to be done


}


\frame
{

\frametitle{Variational inference}

\begin{itemize}

\item Marginal likelhihood 

$$
p(\bfy) = \int \mathcal{N}(\bfy|\bff,\sigma^2 I) 
p(\bff|\bfu, X) p(X) d \bff d \bfu \bfX  
$$
%recall that is invariant to the value of $Z$ 

\item Variational distrbution:
 
$$
q(\bff, \bfu, X) = p(\bff|\bfu, X) \phi(\bfu) q(X)
$$
where 

\begin{itemize}

\item $q(X)=\mathcal{\bfmu,\Sigma}$: Gaussian distribution

\item $\phi(\bfu)$: unrestricted (will turn out to be Gaussian) 

\item $p(\bff|\bfu, X)$: conditonal  GP prior that appears in the joint

\end{itemize}
     

\end{itemize}

\textcolor{red}{The inputs $Z$ are variational parameters, 
e.g.\ affect the form of the variational distribution through $p(\bff|\bfu, X)$}

}



\frame
{

\frametitle{Variational lower bound}

\begin{itemize}

\item Minimization of the $KL[q(\bff,\bfu,X)||p(\bff,\bfu,X|\bfy)]$ is
  equivalent to the maximization of a Jensen's lower bound

\end{itemize}

\begin{multline}
\log p(\bfy) \geq F(q) = \nonumber \\
\int p(\bff|\bfu, X) \phi(\bfu) q(X)  
\log \frac{\mathcal{N}(\bfy|\bff,\sigma^2 I) 
\textcolor{red}{p(\bff|\bfu, X)} p(\bfu) p(X)}
{ \textcolor{red}{p(\bff|\bfu, X)} \phi(\bfu) q(X)} d \bff d \bfu \bfX  
\end{multline}

%\begin{itemize}

%\end{itemize}

%\item 

where $p(\bff|\bfu, X)$s in the $\log$ cancel out and things simplify: 




%\end{itemize}

%\begin{align*} 
%& F(q)  \geq \\
%& \int \phi(\bfu)  \left[
% \langle \log \mathcal{N}(\bfy| \bfalpha, \sigma^2 I) \rangle_{q(X)} + 
%  \log \frac{p(\bfu)}{\phi(\bfu)} 
%\right] d \bfu \\ 
%& -  
% \frac{1}{2 \sigma^2} \text{tr} \left( \langle K_{NN} \rangle_{q(X)} \right)
%+ \frac{1}{2\sigma^2} \text{Tr} \left( K_{MM}^{-1} \langle K_{MN}
%K_{NM} \rangle_{q(X)} \right),    
%\end{align*}


}



\frame
{

\frametitle{Variational lower bound}


\begin{align*} 
& F(q)  \geq  \int \phi(\bfu)  \left[
 \langle \log \mathcal{N}(\bfy| \bfalpha, \sigma^2 I) \rangle_{q(X)} + 
  \log \frac{p(\bfu)}{\phi(\bfu)} 
\right] d \bfu \\ 
& -  
 \frac{1}{2 \sigma^2} \text{tr} \left( \langle K_{NN} \rangle_{q(X)} \right)
+ \frac{1}{2\sigma^2} \text{Tr} \left( K_{MM}^{-1} \langle K_{MN}
K_{NM} \rangle_{q(X)} \right),    
\end{align*}

where $\langle \cdot \rangle_{q(X)}$ expectation under  $q(X)$

\begin{itemize}

\item Define the $\Psi$ statistics 

\begin{itemize}

\item $\psi_0 = \text{tr} \left( \langle K_{NN} 
\rangle_{q(X)} \right)$ (variacne term)

\item $\Psi_1 = \langle K_{NM} \rangle_{q(X)}$ (mean term)

\item $\Psi_2 = \langle K_{MN} K_{NM} \rangle_{q(X)}$
  (covariacne/interaction term)

\item invlove convolution of the Gaussian $q(X)$ with the kernel

\item Closed-forms for squared exponential and linear kernels 

\end{itemize}
  

\end{itemize} 


}







\frame
{

\frametitle{Variational lower bound}

\begin{itemize}

\item Analytically maximize the bound w.r.t.\ $\phi(\bfu)$: 

\end{itemize} 

$$
\widetilde{F}(q) = \log \left( \int e^{ \langle \log \mathcal{N}(\bfy| \bfalpha,
\sigma^2 I) \rangle_{q(X)}} p(\bfu) d \bfu \right)
- \frac{\psi_0}{2\sigma^2}
+ \frac{1}{2\sigma^2}  \text{tr} \left( K_{MM}^{-1} \Psi_2 \right) 
$$

\begin{itemize} 

\item Final form 

\end{itemize}

\begin{multline}
F(q) =   \log \frac{1}{(2 \pi \sigma^2)^{\frac{N-M}{2}})}\frac{|K_{MM}|^{\frac{1}{2}}}
{|\Psi_2 + \sigma^2 K_{MM}|^{\frac{1}{2}}} 
- \frac{1}{2 \sigma^2} \bfy^T \bfy \nonumber \\
+  \frac{1}{2 \sigma^2} \bfy^T \Psi_1 (
\Psi_2 + \sigma^2 K_{MM})^{-1} \Psi_1^T \bfy 
 -  \frac{\psi_0}{2 \sigma^2}  +  
\frac{1}{2\sigma^2} \text{tr} \left(K_{MM}^{-1} \Psi_2 \right),
\end{multline}

\textcolor{red}{Same form with the stardard sparse
 GP variational bound (Titsias, 2009). Now 
the kernel quantities containing $X$ have been replaced 
by variational averages}

}


\frame
{

\frametitle{Variational lower bound}



\begin{multline}
F(q) =   \log \frac{1}{(2 \pi \sigma^2)^{\frac{N-M}{2}})}\frac{|K_{MM}|^{\frac{1}{2}}}
{|\Psi_2 + \sigma^2 K_{MM}|^{\frac{1}{2}}} 
- \frac{1}{2 \sigma^2} \bfy^T \bfy \nonumber \\
+  \frac{1}{2 \sigma^2} \bfy^T \Psi_1 (
\Psi_2 + \sigma^2 K_{MM})^{-1} \Psi_1^T \bfy 
 -  \frac{\psi_0}{2 \sigma^2}  +  
\frac{1}{2\sigma^2} \text{tr} \left(K_{MM}^{-1} \Psi_2 \right),
\end{multline}

It is maximzied w.r.t.\ 

\begin{itemize}

\item Inducing inputs $Z$ (variational parameters) 

\item Variational dist. q(X), i.e.\ $\bfmu$ and $\Sigma$
     (variational parameters)

\item Model hyperpramaters, i.e.\ the parameters of the kernel
  fucntion and noise variance $\sigma^2$   


\item Gradient-based optimization is used, e.g.\ scaled conjugate gradients


\end{itemize}


}




\frame
{
\frametitle{Gaussian process latent variables model (Lawrence, 2005)}


\begin{columns}
\begin{column}[t]{8cm}

\begin{itemize}
\item Latent variable models: 

$$
\bfy  = \bff(\bfx)  + \bfepsilon 
$$

\begin{itemize}

\item $\bfy \in \mathbbm{R}^D$: observed variable
\item $\bfx \in \mathbbm{R}^Q$ ($Q \ll D$): latent variable 
\item $\bff : \mathbbm{R}^Q \rightarrow \mathbbm{R}^D$: latent mapping 

\item GP-LVM: GP priors on the latent mapping

\end{itemize}
\end{itemize}

\end{column}

\begin{column}[t]{4cm}
\begin{figure}
%\begin{l}
\includegraphics[width=25mm,height=33mm]{Figure1.pdf}
%\end{l}
\end{figure}

\end{column}
\end{columns}

\vspace{0.3cm}

GP-LVM is trained by optimizing 
      (not marginalizing out) the latent variables

\begin{itemize}

\item Not proper density in the latent space 

\item Nannot select the latent dimensionality $Q$ 

\item It may overfit, since not fully Bayesian 

\end{itemize}


}



\frame
{
\frametitle{Bayesian Gaussian process latent variables model}


\begin{columns}
\begin{column}[t]{8cm}

\begin{itemize}
\item Latent variable model: 

$$
\bfy  = \bff(\bfx)  + \bfepsilon 
$$

\item Integrate out both the latent mapping 
      and the latent space  

\begin{itemize}

\item Exact Bayesian inference is intractable 

\item But variational Bayesian inference is tractable 
 

\end{itemize}
\end{itemize}

\end{column}

\begin{column}[t]{4cm}
\begin{figure}
%\begin{l}
\includegraphics[width=25mm,height=33mm]{Figure1.pdf}
%\end{l}
\end{figure}

\end{column}
\end{columns}

\vspace{0.5cm}
 

\textcolor{red}{The variational method is applied as before. The only 
      difference is that now we have $D$ latent functions 
      (one for each observed output)
      and not just one}  

}




\frame
{
\frametitle{Bayesian Gaussian process latent variables model}


Automatic learning of the latent dimensionality 

\begin{itemize} 



\item Squared exponential ARD kernel

$$
k(\bfx,\bfx') = \sigma_f^2 \exp\left( - \frac{1}{2} \sum_{q=1}^Q
\alpha_q (x_q - x_q')^2 \right)
$$


\item Maxiziming the variational  lower bound w.r.t.\ 
      $\alpha_q$ allows to prone (shrink to zero) 
      redundant latent dimensions   

\end{itemize}

}


\frame
{
\frametitle{Experimetns with Bayesian GP-LVM}

Oil flow data: $1000$ training, $12$ dimensions, 3 known classes

%\begin{figure*}[ht]
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=34mm,height=30.5mm]
{demOilVargplvmLengthScales1.pdf}&
\includegraphics[width=34mm,height=30.5mm]
{demOilVargplvm1.pdf} &
\includegraphics[width=34mm,height=30.5mm]
{demOilFgplvm7.pdf}\\
($\alpha_q$s) & (Bayesian GP-LVM) & ( GP-LVM)
\end{tabular}
%\caption{Panel (a) shows the inverse lengthscales found by applying the
%  Bayesian GP-LVM with ARD SE kernel on the oil flow data. Panel (b)
%  shows the visualization achieved by keeping the most dominant latent
%  dimensions (2 and 3) which have the largest inverse lengthscale
%  value. Dimension 2 is plotted on the
%  $y$-axis and 3 and on the $x$-axis. Plot (c) shows the visualization found
%  by standard sparse GP-LVM.
%\label{fig:Oil}}
\end{center}
%\end{figure*}

\begin{itemize}

\item Bayesian GP-LVM with 10 latent dimensions

\item The red, green and blue points are the means of the variational
  distribution $q(X)$ (the mean prediction for the latent variables)   

\item $7$ out $10$ latent are shirnk to zero 

\end{itemize}   

}



\frame
{
\frametitle{Experimetns with Bayesian GP-LVM}

Inducing inputs $Z$ (pink diamonds)

%\begin{figure}[ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=50mm,height=40mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/temp/demOilVargplvm1WithInduc.pdf}\\
(inverse lengthscales: $\alpha_q$s)
%\caption{This plot shows the values of the inverse lengthscales
%found by using the Bayesian GP-LVM with ARD SE kernel in Frey faces.  
%\label{fig:BrendanInverseLenghScale}}
\end{tabular}
\end{center}
%\end{figure}


\begin{itemize}

\item Inducing inputs are forced to be close to the means of the
  variational distribution $q(X)$, i.e.\ close to the GP inputs

\item This is consequence of the variarional method 
      for standard sparse GPs (Titsias, 2009)

\end{itemize}   

}





\frame
{
\frametitle{Experimetns with Bayesian GP-LVM}

Fray faces: $1965$ images, $28 \times 20 =560$ dimensions

%\begin{figure*}[ht]
\begin{center}
\begin{tabular}{cccccc}
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag1_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag2_3.pdf} &
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag4_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag11_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag24_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag51_3.pdf}\\
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag1WithMissing_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag2WithMissing_3.pdf} &
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag4WithMissing_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag11WithMissing_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag24WithMissing_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag51WithMissing_3.pdf}\\
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag1Reconst_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag2Reconst_3.pdf} &
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag4Reconst_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag11Reconst_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag24Reconst_3.pdf}&
\includegraphics[width=15mm,height=13mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanTestImag51Reconst_3.pdf}
\end{tabular}
%\caption{Examples of reconstruction of partially observed test images 
%in Frey faces by applying the Bayesian GP-LVM. Each column corresponds to a test
%image. In every column, the top panel shows the true test image, 
%the middle panel the partially observed image (where missing pixels are
%shown in black) and the bottom image is the reconstructed image. 
%\label{fig:BrendanFaces}}
\end{center}
%\end{figure*}

\begin{itemize}

\item Train with 1000 images; reconstuct the missing pixels ($50\%$)
  in the remaining 965 test images 

\item Row 1: original test images. Row 2: image with missing
  pixels given to the algorithm. Row 3: reconstruct images

\end{itemize}   



}




\frame
{
\frametitle{Experimetns with Bayesian GP-LVM}

Fray faces: $1965$ images, $28 \times 20 =560$ dimensions

%\begin{figure}[ht]
\begin{center}
\begin{tabular}{c}
\includegraphics[width=50mm,height=40mm]
{/usr/local/michalis/mlprojects/vargplvm/tex/diagrams/demBrendanVargplvmLengthScales3.pdf}
%\caption{This plot shows the values of the inverse lengthscales
%found by using the Bayesian GP-LVM with ARD SE kernel in Frey faces.  
%\label{fig:BrendanInverseLenghScale}}
\end{tabular}
\end{center}
%\end{figure}



\begin{itemize}

\item Bayesian GP-LVM trained with 30 latent dimensions 
     mean absolute reconstruction error: $7.4003$


\item Standard GP-LVM trained with several latent dimensions:
      $Q= 2, 5, 10, 30$. Errors: $10.5748, 9.7284, 19.6949, 19.6961$

\end{itemize}   



}

\frame
{
\frametitle{Summary}


to be done


}


\end{document}