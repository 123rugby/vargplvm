


%---------------------------------- INTRODUCTION ------------------------------------------------------------------
\onecolumn


\section{Supplementary material}

\subsection{Detailed derivation of the variational lower bound}

\par As explained in the main paper, we assume Gaussian process priors on the mappings, so that:
\begin{align}
f^Y \sim \mathcal{GP}(\bfz, k^Y) & \Rightarrow p(F^Y|X, \bftheta^Y) = \prod_{d=1}^{D_Y} \mathcal{N}(\bff^Y_d | \bfz, K^Y) \nonumber \\
f^Z \sim \mathcal{GP}(\bfz, k^Z) & \Rightarrow p(F^Z|X, \bftheta^Z) = \prod_{d=1}^{D_Z} \mathcal{N}(\bff^Z_d | \bfz, K^Z) \label{GPpriorsSuppl} ,
\end{align}
where $K^{\{Y,Z\}}  = k^{\{Y,Z\}}(\bfx_i, \bfx_j)$ are the covariance matrices evaluated at the latent points.


\par The first step in defining a Bayesian training procedure, is to place a prior distribution $p(X|\bftheta_x)$ over the latent space,
where $\bftheta_x$ denotes any parameters associated with this prior.
For the moment we will not assume any particular form for this distribution and we will omit the conditioning on $\bftheta_x$. Then, the
joint distribution of the model is written as
\begin{align}
p(Y,Z,F^Y,F^Z, X) = & p(Y|F^Y) p(F^Y|X) p(Y|F^Z) p(F^Z|X) p(X) \nonumber \\
		  = & p(X) \prod_{d=1}^{D_Y}  p(\bfy_d | \bff^Y_d) p(\bff^Y_d | X)
			   \prod_{d=1}^{D_Z}  p(\mathbf{z}_d | \bff^Z_d) p(\bff^Z_d | X) , \label{jointSuppl}
\end{align}

\noindent where we assume independence in the data features given the latent variables.
Then, we seek to optimize the model by computing the marginal likelihood

\begin{equation}
\label{marginalLikelihood}
p(Y,Z) =  \int p( Y | F^Y ) p(F^Y | X ) p( Y | F^Z ) p(F^Z | X ) p(X) \intd  X \intd F^Y \intd F^Z.
\end{equation}
 
\noindent The key difficulty with this Bayesian approach is propagating the prior
density $p(X)$ through the nonlinear mapping. This
mapping gives the expressive power to the model, but simultaneously
renders the associated marginal likelihood \eqref{marginalLikelihood} intractable. 

\par We now invoke the variational Bayesian methodology to
approximate the integral. Following a standard variational inference procedure,
 we introduce a variational distribution which we assume to factorise as $q(\Theta)q(X)$ 
where $q(X) \sim \mathcal{N}(\bfmu, S)$. We now
compute the Jensen's lower bound $\mathcal{F}_v$ on the logarithm of
\eqref{marginalLikelihood},
%
\begin{align}
\mathcal{F}_v(q, \boldsymbol \theta) = {}&
  \int q(\Theta)q(X) \log 
	\frac{ p(Y,Z |X)}
		 {q(\Theta)q(X)}  \intd  X \nonumber \\
= {}& 
  \int q(\Theta)q(X) \log \left(
	\frac{p( Y | F^Y ) p(F^Y | X ) p( Y | F^Z ) p(F^Z | X )}{q(\Theta)}
        \frac{p(X)}{q(X)} \right)
  \intd  X \intd F^Y \intd F^Z \nonumber \\
 ={}&
      \int q(\Theta) q(X) \log \frac{p( Y | F^Y ) p(F^Y | X )}{q(\Theta)} \intd F^Y \intd X  \nonumber \\
 +{}& \int q(\Theta) q(X) \log \frac{p( Y | F^Z ) p(F^Z | X )}{q(\Theta)} \intd F^Z \intd X  \nonumber \\
 -{}& \int \cancel{q(\Theta)} q(X) \log \frac{q(X)}{p(X)} \nonumber \\
={}& \mathcal{L}_Y + \mathcal{L}_Z - \text{KL}[q(X) \parallel p(X)]
		 \label{jensens1Suppl}
\end{align}
%
where $\boldsymbol \theta$ denotes the model's parameters $\bftheta = \{\bftheta^Y, \bftheta^Z\}$.  However,
the above form of the lower bound is problematic because $X$ (in the
GP terms $p(F^Y|X)$ and $p(F^Z|X)$) appears non-linearly inside the kernel matrices
$K^Y$ and $K^Z$ of equation \eqref{GPpriorsSuppl}, making the integration over $X$ difficult.
It is, thus, obvious that standard mean field variational methodologies do not lead to an analytically
tractable algorithm.
\par In contrast, our framework allows us to compute
a closed-form Jensen's lower bound
by applying variational inference after expanding the GP prior so as to include auxiliary inducing
variables. Originally, inducing variables were introduced for computational speed ups in GP regression models.
 In our approach, these extra variables are used as in the variational sparse GP method of Titsias
\cite{Titsias:variational09}.
More specifically, we expand the joint
 probability model in (\ref{jointSuppl}) 
 with $M$ extra samples $U^Y$ and $U^Z$ of the latent functions $f^Y$ and $f^Z$ evaluated at a set of pseudo-inputs
(known as ``inducing points'')
$\bar{X}^Y$ and $\bar{X}^Z$ respectively. Here, 
$U^Y \in \mathbb{R}^{M_Y \times D_Y}$, $U^Z \in \mathbb{R}^{M_Z \times D_Z}$,
 $\bar{X}^Y \in \mathbb{R}^{M_Y \times Q}$,  $\bar{X}^Z \in \mathbb{R}^{M_Z \times Q}$
and $M = M_Y+M_Z$.


 % This general strategy is originally used in approximating GP marginal likelihoods, since it allows reducing
 % the computational and storage complexity from $O(N^3)$ to $O(N M^2)$. Certain methods learn the positions of
 % the inducing points by transforming them into additional kernel hyperparameters and optimizing jointly with
 % respect to all the unknown quantities. However, this technique basically changes the model structure, since
 % the GP prior is practically modified.
 % On the other hand, the approach taken here builds on and significantly extends the recent method of
 % \cite{Titsias:variational09}, where the inducing pionts are transformed 

 
 The augmented joint probability density takes the form
 \begin{align}
 p(Y,F^Y, F^Z, U^Y, U^Z,X,\bar{X}^Y,\bar{X}^Z) =
    p(X) & \prod_{d=1}^{D_Y} p(\bfy_d | \bff^Y_d) p(\bff^Y_d | \bfu^Y_d, X) p(\bfu^Y_d | X)  & \nonumber \\
         & \prod_{d=1}^{D_Z} p(\mathbf{z}_d | \bff^Z_d) p(\bff^Z_d | \bfu^Z_d, X) p(\bfu^Z_d | X) \label{augmentedJointSuppl}
 \end{align}
 where $p(\bfu^{\{Y,Z\}}_d | X)$ are zero-mean Gaussians with
 covariance matrices $K^Y_{MM}$ and $K^Z_{MM}$ respectively, constructed using the same functions as for
 the GP priors \eqref{GPpriorsSuppl}. 
 By dropping $X$ from our
 expressions, we write the augmented GP prior analytically (see
 \cite{Rasmussen:book06}) as
 \begin{equation}
  \label{priorF2Suppl}
 p(\bff^Y_d | \bfu^Y_d, X) =  \mathcal{N}  \left( \bff^Y_d | \left(K^Y_{NM} K^Y_{MM}\right)^{-1}
	\bfu^Y_d , K^Y_{NN} - K^Y_{NM} \left(K^Y_{MM}\right)^{-1} K^Y_{MN} \right),
 \end{equation}
 and similarly for $Z$. Here, $K^Y_{NN} = K^Y(X,X)$ and $K^Y_{NM}$ denotes the cross-covariance between the function values of $k^Y$ evaluated at
the latent points $X$ and the inducing points $\bar{X}^Y$.

\noindent 
\par Analogously to \cite{Titsias:bayesGPLVM10}, we are now able to obtain a tractable lower bound through
the variational density:
\begin{equation}
\label{qThetaSuppl}
q(\Theta)q(X) = \{q(U^\mathcal{K}) p(F^\mathcal{K}|U^\mathcal{K},X,\bar{X}^\mathcal{K})\}_{\mathcal{K}=\{Y,Z\}} q(X),
\end{equation} 
where $q(U^{\{Y,Z\}})$ are free form distributions and $q(X)$ a Gaussian with parameters $\bfmu$ and $S$.
Notice also that \eqref{qThetaSuppl} factorises across dimensions.
Optimization of the variational lower bound provides an approximation to the true posterior $p(X|Y,Z)$
by $q(X)$. 

\par After defining a variational distribution, we can continue our derivation by returning to
the expression for the lower bound \eqref{jensens1Suppl} and replacing the joint distribution with
 its augmented version \eqref{augmentedJointSuppl} and the variational distribution with its
 factorised version \eqref{qThetaSuppl}. Since the variational bound breaks to separate terms for each
of the observations spaces, here we will drop the subscripts $Y$ and $Z$ and show how, in general, we
can calculate the $\mathcal{L}$ terms of equation \eqref{jensens1Suppl} for a general observation space $Y$. We have:

\begin{align}
\mathcal{L} = {}& \int q(\Theta)q(X) \log 
		\frac{ p(Y,F, U | X, \bar{X})}
			 {q(\Theta)}  \intd  X \intd F \intd U  \nonumber \\
= {}& \int \prod_{d=1}^D p(\bff_d | \bfu_d, X , \bar{X})q(\bfu_d)
	    \log  \frac{\prod_{d=1}^D p(\bfy_d | \bff_d) \cancel{p(\bff_d | \bfu_d, X, \bar{X})}
						p(\bfu_d | \bar{X})}
 	 {\prod_{d=1}^D \cancel{p(\bff_d | \bfu_d, X, \bar{X} )}q(\bfu_d) q(X)}   \intd  X \intd F \intd U  \nonumber \\
 ={}& \int \prod_{d=1}^D p(\bff_d | \bfu_d, X, \bar{X} )q(\bfu_d) q(X) 
		\log  \frac{\prod_{d=1}^D p(\bfy_d | \bff_d) p(\bfu_d | \bar{X})}
				   {\prod_{d=1}^D q(\bfu_d) q(X))}   \intd  X \intd F \intd U 
\end{align}

Dropping $\bar{X}$ from our expressions, for simplicity, we finally obtain:
\begin{equation}
\label{LSuppl}
\mathcal{L} =
\sum_{d=1}^D \left( 
    \int q(\bfu_d) q(X) \left\langle \log p(\bfy_d | \bff_d) \right\rangle_{p(\bff_d | \bfu_d, X)} \intd \bfu_d \; \intd X +
					   \log \left\langle \frac{p(\bfu_d)}{q(\bfu_d)} \right\rangle_{q(\bfu_d)} 
  \right) = \sum_{d=1}^D \mathcal{L}_d .
\end{equation} 


\noindent Calculating \eqref{LSuppl} in the same manner for every available observation space and 
replacing back in the variational bound \eqref{jensens1Suppl} 
we obtain the final form of the bound which is now analytically tractable. 
In particular, the $\text{KL}$ term
%depends on the prior and its calculation
is tractable and easy for certain priors $p(X)$.
As for the $\mathcal{L}$ terms, we can calculate the
expectation over $p(\bff_d | \bfu_d,X)$ and reveal that the optimal setting for $q(\bfu_d)$ is 
also a Gaussian. More specifically, we have:
\begin{align}
\mathcal{L}_d={}& \int q(\bfu_d) \log \frac{e^{\la \log N \left( \bfy_d | \bfa_d, \beta^{-1} I_d \right) \ra_{q(X)}}
		p(\bfu_d)}{q(\bfu_d)} \intd \bfu_d - \mathcal{A} , \label{boundFAnalytically5}
\end{align}
where $\bfa_d$ is the mean of \eqref{priorF2Suppl} and 
$\mathcal{A}=\frac{\beta}{2} \text{Tr}(\la K_{NN} \ra_{q(X)}) +
	 	\frac{\beta}{2} \text{Tr} \left(K_{MM}^{-1} \la K_{MN} K_{NM} \ra_{q(X)} \right) $.
The expression in \eqref{boundFAnalytically5} is a KL-like quantity and, therefore, $q(\bfu_d)$ is optimally set to be the quantity 
appearing in the numerator of the above equation. So:

\begin{equation}
\label{qu}
q(\bfu_d) = e^{\la \log \mathcal{N} \left( \bfy_d | \bfa_d, \beta^{-1} I_d \right) \ra_{q(X)}}
		p(\bfu_d) ,
\end{equation}
exactly as in \cite{Titsias:bayesGPLVM10}. This is a Gaussian distribution since we have assumed
$p(\bfu_d ) = \mathcal{N} (\bfu_d | \mathbf{0}, K_{MM} )$.

\par After replacing $q(\bfu_d)$ with its optimal value, we can reverse Jensen's inequality to obtain:

\begin{equation}
\label{boundFAnalyticallyFinalIntegralSuppl}
\mathcal{L}_d \geq
	\log \int e^{\la \log N \left( \bfy_d | \bfa_d, \beta^{-1} I_d \right) \ra_{q(X)}}
		p(\bfu_d) \intd \bfu_d -\mathcal{A} .
\end{equation}

\noindent Notice that the expectation appearing above is a standard Gaussian integral and \eqref{boundFAnalyticallyFinalIntegralSuppl} can
be calculated in closed form, which turns out to be:
\begin{equation}
\label{LFinalSuppl}
\mathcal{L}_d(q, \boldsymbol \theta) \geq \log \left[ 
	\frac{(\beta)^{\frac{N}{2}} \vert \mathit{K_{MM}} \vert ^\frac{1}{2} }
		 {(2\pi)^{\frac{N}{2}} \vert \beta \Psi_2 + \mathit{K_{MM}}  \vert ^\frac{1}{2} } 
	 e^{-\frac{1}{2} \bfy^{T}_{d} W \bfy_d}
	 \right]	 -
	 \frac{\beta \psi_0}{2} + \frac{\beta}{2} 
	 \text{Tr} \left( \mathit{K_{MM}^{-1}} \Psi_2 \right)	
\end{equation}

\noindent where:
\begin{equation}
\label{psis}
\Psi_0 = \text{Tr}(\langle \mathit{K_{NN}} \rangle_{q(X)}) \;, \;\;
\Psi_1 = \langle \mathit{K_{NM}} \rangle_{q(X)} \;, \;\;
\Psi_2 = \langle \mathit{K_{MN}} \mathit{K_{NM}} \rangle_{q(X)}
\end{equation}

\noindent and $W = \beta I_N - \beta^2 \Psi_1 (\beta \Psi_2 + K_{MM})^{-1} \Psi_1^T$. This expression is straight forward to compute, as long as the covariance functions $k^Y$ and $k^Z$
 are selected so that the $\Psi$ quantities of \eqref{psis} can be computed analytically. As shown in \cite{Titsias:bayesGPLVM10}, these
statistics constitute convolutions of the covariance function with Gaussian densities and 
are tractable for many standard covariance functions, such as the ARD squared exponential or the linear one.
%Indeed, we can write
%$\psi_0 = \sum_{n=1}^N \psi_)^n$, with
%\begin{align}
%\psi_0^n = \int k(\bfx)
%\end{align}


% 
% \par
% %It is now obvious how automatic selection of the latent space dimensionality can be achieved
% It is now obvious how our models are able to find automatically an
% 
% It is now obvious how our models
% are able to find automatically an effective number of dimensions for the latent space:
% when the ARD covariance function \eqref{ard} (or the linear ARD function defined in \eqref{linard}) are used for the mapping
% $k_f$, the parameter vector $\bftheta_f$ includes the $Q$ different scales $w_q$,
% and the Bayesian training is able to ``switch off'' unnecessary dimensions by driving the values of the corresponding
% scales to, or close to zero. Notice that this is only possible in a Bayesian framework such as the one presented here and
% opting for an ARD covariance function for the standard GP-LVM would not cause the same effect. This is because in the standard
% GP-LVM, the number of parameters increases proportionally to the number of latent dimensions, since the training procedure
% relies on maximising $p(Y|X)$. Therefore, the likelihood cannot provide any insight for the optimal number of latent dimensions,
% since it always increases when more dimensions are added.


\subsection{Inferring a new latent point}

Given a model which is trained so as to jointly represent two output spaces $Y$ and $Z$ with
a common but factorised input space $X$, we wish to generate a new (or infer a training) set of outputs
$Z^* \in \mathbb{R}^{N^* \times D_Z}$ given a set of (potentially partially) observed test points $Y^* \in \mathbb{R}^{N^* \times D_Y}$.
This is done in three steps, as explained in the main paper. Here we explain in more detail the first step, where we need to predict
 the set of latent points $X^* \in \mathbb{R}^{N^* \times Q}$
which is most likely to have generated $Y^*$. 
\par To achieve this, we use an approximation to the posterior $p(X^*|Y^*,Y)$,
which has the same form as for the standard Bayesian GP-LVM model \cite{Titsias:bayesGPLVM10} and is given by a variational distribution
 $q(X,X^*)$. To find $q(X,X^*)$ we optimise a variational lower bound $\mathcal{F}_v = \mathcal{F}^*_v \left(q(X,X^*) \right)$
on the marginal likelihood $p(Y,Y^*)$ which has analogous form with the training
  objective function \eqref{eq:bound}. In specific, we ignore $Z$ and replace $Y$ with $(Y,Y^*)$ and $X$ with $(X,X^*)$ in \eqref{eq:bound}
so as to get:
\begin{align}
 \mathcal{F}^*_v 
   & = \int p(Y^*,Y | X^*,X) p(X^*,X) \intd X^* \intd X \nonumber \\
   & \leq \int q(X^*,X) q(\Theta) \log 
      \frac{p(Y^*,Y | X^*,X) p(X^*,X)}{q(X^*,X) q(\Theta)} \intd X \intd X^* .
\end{align}
What now remains is to define $q(X^*,X)$. At this step, the inference procedure differs depending on the type of prior used for the latent space $X$.
Specifically, if we use a prior that does not couple datapoints, such as a standard normal one, then we are allowed to
write that $q(X,X^*) = \prod_{n=1}^N q(\bfx_n) \prod_{n=1}^{N^*} q(\bfx_n^*)$, 
where $q(\bfx_n^*) = \mathcal{N}(\bfx_n^* | \bfmu_n^*, S_n^*)$.

\par On the other hand, performing inference in the dynamical model is more challenging, since $q(X^*,X)$ is fully
coupled across $X$ and $X^*$. Therefore, if we wish to maintain the correlation of the inputs depending on their times,
we should select this distribution to only factorise across features: 
$q(X^*,X) = \prod_{q=1}^Q  \mathcal{N} (\bfx_q^* | \bfmu_q^* ,S_q^*)$,
 where $S_{q,n}$ are $(N+N^*) \times (N+N^*)$ matrices.


