\section{Experiments \label{experiments}}
The MRD method is designed to represent multiple views of a data set
as a set of factorized latent spaces. In this section we will show
experiments which exploit this factorized structure. Source code for
recreating these experiments is included as supplementary material.

%learn the shared and private spaces 
%of distinct datasets which, nevertheless, have some underlying commonality.
%For this reason, for each experiment we consider \emph{pairs}
% of (possibly heterogeneous) datasets, although the model can as well be applied
%to more than two subsets.
%The various properties of the model are explored by considering different tasks (visualisation, datapoint correspondence,
%data generation, classification) and datasets that are different in nature.
%
%The three pairs of datasets considered here are different in nature, 
%allowing us to explore the various properties of our method. 
%
%Firstly,
%we consider a set of very high dimensional images belonging two six different human faces, spread into two datasets. The principal underlying
%commonality of the two subsets, which our method effectively discovers, is the varying light condition of the images. As a second
%experiment, we opted for a set of recordings of various human motions which is provided in two different representations: a subset containing %pose data and a subset containing the
%the corresponding silhouette features. 
%
%The performance of the model is evaluated in different tasks, such as visualisation and interpretation
%of the latent space which is discovered and segmented automatically, correspondence of datapoints between the two subsets of the given
%datasets, as well as generation of new data and classification.
%
%Source code for recreating these experiments is included as supplementary material.

%\subsection{Yale faces dataset}
\noindent{\bf Yale faces:}
To show the ability of our method to model very
high-dimensional spaces our first experiment is applied to the Yale
dataset \cite{YaleFaces1, YaleFaces2} which contains images of several
human faces under different poses and 64 illumination conditions.  We
consider a single pose for each subject such that the only variations
are the location of the light source and the subject's appearance.
Since our model is capable of working with very high-dimensional data, %. This means that
it can be directly applied to the raw pixel values (in this
case $192 \times 168 = 32,256$ pixels/image) so that we do not
have to rely on image feature extraction to pre-process the data, and we can directly sample novel outputs.
%% to
%% demonstrate the ability of our method to model data with a very large
%% number of features.  With this approach, we can also directly sample
%% new images from the learned model.
% \subsubsection{Modeling one face}
%Before we proceed to subspace modelling, we first fit the standard Bayesian GP-LVM model to the whole set of $64$ images belonging to a single face, to visualise and assess the quality of the discovered latent space.
%The model was initialised with $Q=15$ latent dimensions, and the Bayesian training not only discovered the effective dimensionality of the latent space automatically,
%but it also defined the ``importance'' of each dimension. As can be seen in figure \ref{fig:yaleOneFaceScales}, most of the mapping kernel's weights were driven close to zero, signifying that the 
%latent space is dominated by the three dimensions which have been assigned large weights.
%
%In figures
%\ref{fig:yaleOneFaceX21} and \ref{fig:yaleOneFaceX23}, one can see that the projection of the latent space into the 3 most dominant dimensions is shaped as a hollow 
%hemishpere, which is in accordance with the shape of the space defined by the fixed locations of the light source.
%
%\hspace{-6pt}
%\begin{figure}[ht]
%\begin{center}
%\subfigure[]{
%\includegraphics[width=0.145\textwidth]{../diagrams/Yale1Face/scales}
%	\label{fig:yaleOneFaceScales}
%}
%\hspace{-5pt}
%\subfigure[]{
%	\includegraphics[width=0.145\textwidth]{../diagrams/Yale1Face/X21}
%	\label{fig:yaleOneFaceX21}
%}
%\hspace{-5pt}
%\subfigure[]{
%	\includegraphics[width=0.145\textwidth]{../diagrams/Yale1Face/X23}
%	\label{fig:yaleOneFaceX23}
%}
%\end{center}
%\vspace{-7pt}
%\caption{\small{ \it
%The weight set $\bfw$ associated with the learned latent space is shown in \subref{fig:yaleOneFaceScales}.
%In figures \subref{fig:yaleOneFaceX21} and \subref{fig:yaleOneFaceX23} we plotted pairs of the $3$ most dominant
%latent dimensions against each other (dimension 1 against 2 and 1 against 3 respectively).
%}
%}
%\label{fig:yaleOneFace1}
%\vspace{-8pt}
%\end{figure}
%\hspace{-6pt}
%
%This suggests that latent feature indices $1,2$ and $3$ encode the information about the illumination condition.
%Figure \ref{fig:yaleOneFaceScales} also shows that the latent dimensions with indices $4,5$ and $6$ have a very small but not negligible weight.
%%These are retained by the model because
%%apart from the illumination condition, there exist other minor differences among pictures of a single face, as the depicted %persons
%These represent other minor differences between an individual's face pictures, as the subjects
%often blink, slightly move or smile etc. 
%
% \subsubsection{Shared latent spaces for multiple faces}
%In this section we use our model, from now on referred to as \emph{Manifold Relevance Determination model (MRD)} for latent subspace learning. 
%
%To test our model in the Yale face data
From the full Yale database, we constructed a dataset $Y$ containing the pictures corresponding to all $64$ different illumination conditions for each one of $3$
subjects and
%\highlight{Need to be specific about what's in Y and what's in Z,  it's currently not clear!!!}
similarly for $Z$, for $3$ different subjects.
  In this way, we formed two datasets, $Y$ and $Z$,
each consisting of all $64 \times 3$ images corresponding to a set of
three different faces, under all possible illumination conditions, therefore, $Y, Z \in \mathbb{R}^{N \times D}$,
$N = 192$, $D = 32,256$.
 We then aligned the order of the images in each dataset, so that each
image $\bfy_n$ from the first one was randomly set to correspond to one of
the $3$ possible $\bfzi_n$'s of the second dataset which are depicted in the same
illumination condition as $\bfy_n$.
In that way, we matched datapoints between the two datasets only according to the 
illumination condition and not the identity of the faces, so that
the model is not explicitly forced to learn the correspondence between face characteristics.

\par The latent space variational means were initialised by
concatenating the two datasets and performing PCA. An alternative
approach would be to perform PCA on each dataset separately and then
concatenate the two low dimensional representations to initialise $X$.
We found that both initializations achieved similar results. 
The optimized relevance weights $\{\bfw^Y, \bfw^Z\}$
%weights were optimized by maximizing the variational lower bound and 
are
visualized as bar graphs in figure \ref{fig:yale6SetsScales}.
% by the parameters of the generative
%mappings $f^Y$ and $f^Z$ as shown in figure \ref{fig:yale6SetsScales}.

\begin{figure}[ht]
\begin{center}
\subfigure[]{
\includegraphics[width=0.2\textwidth]{../diagrams/Yale6Sets/scalesMod1_noNum}
	\label{fig:yale6SetsScales1}
}
\subfigure[]{
	\includegraphics[width=0.2\textwidth]{../diagrams/Yale6Sets/scalesMod2_noNum}
	\label{fig:yale6SetsScales2}
}
\end{center}
\vspace{-5pt}
\caption{
%\small{ \it
The relevance  weights for the faces data. Despite allowing for soft sharing, the first 3 dimensions are switched on with approximately the same weight for both views of the data. Most of the remaining dimensions are used to explain private variance. %Dimension 9 is switched off.
% in \subref{fig:yale6SetsScales1} and \subref{fig:yale6SetsScales2},
% define a partitioning and ``soft'' sharing of the latent space. 
%}
}
\label{fig:yale6SetsScales}
%\vspace{-8pt}
\end{figure}

The latent space is clearly segmented into a shared part, consisting
of dimensions indexed as $1$,$2$ and $3$ \footnote{Dimension 6
also encodes shared information, but of almost negligible amount ($w^Y_6$ and $w^Z_6$ are almost zero).}
% is also
%  in the shared set but both models assigned a very small weight, as
%  it encodes an almost negligible amount of information.} 
two private and an irrelevant part (dimension $9$).
%, consisting of dimensions indexed as $\{5,7,11,14 \}$
%and $\{4,8,10,12,13 \}$ respectively.
%  The $9$th feature of every
%latent point was found to be unnecessary for the generation of both
%output spaces. 
The two data views allocated approximately equal
weights to the shared latent dimensions, which %. These three latent dimensions
are visualized in figures
\ref{fig:yale6SetsLatentSpace}\subref{fig:yale6SetsX12} and
\ref{fig:yale6SetsLatentSpace}\subref{fig:yale6SetsX13}. Interaction
with these three latent dimensions reveals that the structure of the
shared subspace resembles a hollow hemisphere. This corresponds to the
shape of the space defined by the fixed locations of the light source.
%

%\hspace{-6pt}
\begin{figure}[ht]
\begin{center}
\subfigure[]{
\includegraphics[width=0.145\textwidth]{../diagrams/Yale6Sets/mod1X_1_2}
	\label{fig:yale6SetsX12}
}
\hspace{-5pt}
\subfigure[]{
	\includegraphics[width=0.145\textwidth]{../diagrams/Yale6Sets/mod1X_1_3}
	\label{fig:yale6SetsX13}
}
\hspace{-5pt}
\subfigure[]{
	\includegraphics[width=0.145\textwidth]{../diagrams/Yale6Sets/mod1_X5_14}
	\label{fig:yale6SetsX5_14}
}
\end{center}
\vspace{-4pt}
\caption{
%\small{ \it
Projection of the shared latent space into dimensions $\{1,2\}$ and $\{1,3\}$ (figures
\subref{fig:yale6SetsX12} and \subref{fig:yale6SetsX13}) and projection of the $Y-$private dimensions $\{5,14\}$
(figure \subref{fig:yale6SetsX5_14}).
%which disambiguate between different faces in the first dataset.
 It is clear how the latent points in figure
\subref{fig:yale6SetsX5_14} form three clusters, each responsible for modelling one of the three faces in $Y$.
%}
%The grey scale background indicates the precision with which the mapping is expressed.
}
\label{fig:yale6SetsLatentSpace}
%\vspace{-8pt}
\end{figure}
%\hspace{-6pt}
%
%the shape of the shared latent space is similar to
%the one found by the Bayesian GP-LVM (figure \ref{fig:yaleOneFace1}), as can be seen in figures \ref{fig:yale6SetsLatentSpace}\subref{fig:yale6SetsX12} and
%\ref{fig:yale6SetsLatentSpace}\subref{fig:yale6SetsX13}.
%
This indicates that the shared space successfully encodes the
information about the position of the light source and not the face
characteristics.
%as the random alignment that we performed forbade the algorithm from modelling commonalities between faces of the
%two different datasets.
%
This indication is enhanced by the results found when we performed
dimensionality reduction with the standard Bayesian GP-LVM for
pictures corresponding to all illumination conditions of a single face (\ie a dataset with one
modality).  Specifically, the latent space discovered by the Bayesian
GP-LVM and the shared subspace discovered by MRD have the same
dimensionality and similar structure, as can be seen in figure
\ref{fig:yaleOneFace1}.
%
\hspace{-6pt}
\begin{figure}[ht]
\begin{center}
\subfigure[]{
\includegraphics[width=0.142\textwidth]{../diagrams/Yale1Face/scalesVargplvm_noNum}
	\label{fig:yaleOneFaceScales}
}
\hspace{-5pt}
\subfigure[]{
	\includegraphics[width=0.145\textwidth]{../diagrams/Yale1Face/X21}
	\label{fig:yaleOneFaceX21}
}
\hspace{-5pt}
\subfigure[]{
	\includegraphics[width=0.145\textwidth]{../diagrams/Yale1Face/X23}
	\label{fig:yaleOneFaceX23}
}
\end{center}
\vspace{-4pt}
\caption{
%\small{ \it
Latent space learned by the standard Bayesian GP-LVM for a single face dataset.
The weight set $\bfw$ associated with the learned latent space is shown in \subref{fig:yaleOneFaceScales}.
In figures \subref{fig:yaleOneFaceX21} and \subref{fig:yaleOneFaceX23} we plotted pairs of the $3$ dominant
latent dimensions against each other. Dimensions $4,5$ and $6$ have a very small but not negligible weight and
represent other minor differences between pictures of the same face, as the subjects often blink, smile etc.
%}
}
\label{fig:yaleOneFace1}
%\vspace{-8pt}
\end{figure}
\hspace{-6pt}
%
\par As for the private manifolds discovered by MRD, these correspond
to subspaces for disambiguating between faces of the same dataset.
Indeed, plotting the largest two dimensions of the first latent
private subspace against each other reveals three clusters,
corresponding to the three different faces within the dataset.
Similarly to the standard Bayesian GP-LVM applied to a single face,
here the private dimensions with very small weight model slight changes
across faces of the same subject (blinking etc).
%
%here the private dimensions with the very small weight are the ones
%that model the minor differences introduced by the fact that the
%subject characteristics are slightly changed in several photos (due to
%blinking etc).
%
%The MRD method not only finds a very efficient and intuitive
%factorization of the latent space into lighting conditions and facial
%representation, but this factorization is also found automatically through the
%approximate variational Bayesian algorithm.

We can also confirm visually the subspaces' properties by sampling a
set of novel inputs $X_{samp}$ from each subspace and then mapping
back to the observed data space using the likelihoods $p(Y|X_{samp})$ or $p(Z|X_{samp})$,
thus obtaining novel outputs (images).
%Being able to do so is an important advantage of our method (which is generative), because it also means that we can generate novel datapoints from the trained model.
%The intuitive segmentation of the latent space also helps towards this direction. 
To better understand what kind of information is encoded in each of
the dimensions of the shared or private spaces, we sampled new latent
points by varying only one dimension at a time, while keeping the rest
fixed.
%
The first two rows of figure \ref{fig:yale6SetsInterpolation} show
some of the outputs obtained after sampling across each of the shared
dimensions $1$ and $3$ respectively, which clearly encode the
coordinates of the light source, whereas dimension $2$ was found to
model the overall brightness. The sampling procedure can intuitively
be thought as a walk in the space shown in figure
\ref{fig:yale6SetsLatentSpace}\subref{fig:yale6SetsX13} from left to
right and from the bottom to the top. Although the set of learned
latent inputs is discrete, the corresponding latent subspace is
continuous, and we can interpolate images in new illumination
conditions by sampling from areas where there are no training inputs
(\ie in between the red crosses shown in figure
\ref{fig:yale6SetsLatentSpace}).

% Using a generative model also allows us to sample new datapoints from the trained model. This is done by selecting a training input $\bfx_n$ (\ie one of
% the latent points corresponding to an existing, observed $\bfy_n$), and sampling one or more of its dimensions from the whole of the corresponding feature space,
% obtaining, thus, a novel input $\bfx_*$. We can then obtain novel outputs by using the likelihood $p(\bfy_n | \bfx_n)$. Figures \ref{}, \ref{} and \ref{} show some of the outputs obtained after
% sampling across each of the principle dimensions by hand. When the sampled $\bfx_*$ were close or exactly similar to training inputs, the corresponding output
% looks like one of the training datapoints, otherwise the output is novel. From the aforementioned figures, one can see that the two of the principal dimensions
% represent the change of the light source location along the $X$ and $Y$ axis respectively, whereas the third one is responsible for modelling the brightness.

Similarly, we can sample from the private subspaces and obtain novel
outputs which interpolate the non-shared characteristics of the
involved data.  This results in a morphing effect across different
faces, which is shown in the last row of figure
\ref{fig:yale6SetsInterpolation}.  Example videos can be
found in the supplementary material.


\begin{figure*}[ht]
\begin{center}
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1000} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1009} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1021} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1022} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1036} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1040} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1046} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1055} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1063} }
\vspace{-8pt}
\newline
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1064} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1072} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1079} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1085} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1095} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1110} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1125} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1137} }
\subfigure{ \includegraphics[width=0.09\textwidth]{../diagrams/Yale6Sets/lightInterpolation/X13_1149} }
\vspace{-8pt}
\newline
\subfigure{ \includegraphics[width=0.1\textwidth]{../diagrams/Yale6Sets/morphing/1054} }
\subfigure{ \includegraphics[width=0.1\textwidth]{../diagrams/Yale6Sets/morphing/1079} }
\subfigure{ \includegraphics[width=0.1\textwidth]{../diagrams/Yale6Sets/morphing/1089} }
\subfigure{ \includegraphics[width=0.1\textwidth]{../diagrams/Yale6Sets/morphing/1094} }
\subfigure{ \includegraphics[width=0.1\textwidth]{../diagrams/Yale6Sets/morphing/1102} }
\subfigure{ \includegraphics[width=0.1\textwidth]{../diagrams/Yale6Sets/morphing/1106} }
\subfigure{ \includegraphics[width=0.1\textwidth]{../diagrams/Yale6Sets/morphing/1123} }
\end{center}
%\vspace{-4pt}
\caption{
%\small{ \it
Sampling inputs to produce novel outputs.
First row shows interpolation between positions of the light source in the $x$ coordinate
and second row in the $y$ coordinate (elevation). Last row shows interpolation between
face characteristics to produce a morphing effect. Note that these images are presented scaled here, see suppl. material for the original $32,256$-dimensional ones.
%}
}
\label{fig:yale6SetsInterpolation}
%\vspace{-8pt}
\end{figure*}


\par As a final test, we confirm the efficient segmentation of the
latent space into private and shared parts by automatically recovering
all the illumination similarities found in the training set.  More
specifically, given a datapoint $\bfy_n$ from the first dataset, we
search the whole space of training inputs $X$ to find the $6$ Nearest
Neigbours to the latent representation $\bfx_n$ of $\bfy_n$, based
only on the shared dimensions.
%In other words, we compare $x_{n,i}$ to all the rest $\{x_{n,j}\}_{n=1}^N$, where $i \neq j$ and $i,j$ belong to the set
% of the shared dimensions. 
 From these latent points, we can then obtain points in the output
 space of the second dataset, by using the likelihood $p(Z | X)$.  As
 can be seen in figure \ref{fig:yale6SetsGrouping}, the model returns
 images with matching illumination condition.  Moreover, the fact
 that, typically, the first neighbours of each given point correspond
 to outputs belonging to different faces, indicates that the shared
 latent space is ``pure'', and is not polluted by information that
 encodes the face appearance.
%\hspace{-6pt}
\begin{figure}[ht]
\begin{center}
\subfigure{ \includegraphics[width=0.47\textwidth]{../diagrams/Yale6Sets/grouping/givenMod2/122} }
 \vspace{-16pt}
 \newline
\subfigure{ \includegraphics[width=0.47\textwidth]{../diagrams/Yale6Sets/grouping/givenMod2/107} }
 \vspace{-16pt}
 \newline
\subfigure{ \includegraphics[width=0.47\textwidth]{../diagrams/Yale6Sets/grouping/givenMod1/24} }
 \vspace{-16pt}
 \newline
\subfigure{ \includegraphics[width=0.47\textwidth]{../diagrams/Yale6Sets/grouping/givenMod1/70} }
 \vspace{-16pt}
 \newline
\end{center}
\vspace{-7pt}
\caption{
%\small{ \it
Given the images of the first column, the model searches only in the shared latent space to find the pictures of the opposite dataset
which have the same illumination condition. The images found, are sorted in columns
$2$ - $7$ by relevance.
%}
}
\label{fig:yale6SetsGrouping}
\vspace{-8pt}
\end{figure}

%\hspace{-6pt}

%%%% THE FIRST NN is the same latentn point, we should look from NN2 and so on.


%\subsection{Human motion data}
\noindent{\bf Human motion data:}
For our second experiment, we consider a set of $3$D human poses and
associated silhouettes, coming from the dataset of Agarwal and Triggs
\cite{Agarwal:pose06}. We used a subset of $5$ sequences, totalling
$649$ frames, corresponding to walking motions in various directions
and patterns.  A separate walking sequence of $158$ frames was used as
a test set.  Each pose is represented by a $63-$dimensional vector of
joint locations and each silhouette is represented by a
$100-$dimensional vector of HoG (histogram of oriented gradients) features.

\par Given the test silhouette features, we used our model to generate
the corresponding poses. This is challenging, as the data
are multi-modal, \ie a silhouette representation may be generated from
more than one poses (\eg fig. \ref{fig:humanPoseAmbiguity}).

\begin{figure}[ht]
\begin{center}
%  \includegraphics[width=0.43\textwidth]{../diagrams/humanPose/ambiguity2FinalCombined3}
  \includegraphics[width=0.3\textwidth]{../diagrams/humanPose/newAmbiguity}
\end{center}
\vspace{-7pt}
\caption{
%\small{ \it
Although the two poses in the second column are very dissimilar, they correspond to resembling silhouettes
that have similar feature vectors. This happens because the $3$D information is lost in the silhouette space,
as can also be seen in the third column, depicting the same poses from the silhouettes' viewpoint.
%}
}
\label{fig:humanPoseAmbiguity}
%\vspace{-4pt}
\end{figure}

As described in the inference section, given $\bfy^*$, one of the
$N^*$ test silhouettes, our model optimises a test latent point
$\bfx^*$ and finds a series of $K$ candidate initial training inputs $
\{ \bfx_{NN}^{(k)} \}_{k=1}^K$, sorted according to their similarity
to $\bfx^*$, taking into account only the shared dimensions.  Based on
these initial latent points, it then generates a sorted series of $K$ \emph{novel}
poses $\{ \bfzi^{(k)} \}_{k=1}^K$. For the dynamical version of our
model, all test points are considered together and the predicted
$N^*$ outputs are forced to form a smooth sequence.  Our experiments
show that the initial training inputs $\bfx_{NN}$ typically
correspond to silhouettes similar to the given one, something which
confirms that the segmentation of the latent space is
efficient. However, when ambiguities arise, as the example shown in
figure \ref{fig:humanPoseAmbiguity}, the non-dynamical version of our
model has no way of selecting the correct input, since all points of the
test sequence are treated independently. But when the dynamical
version is employed, the model forces the whole set of training and
test inputs to create smooth paths in the latent space.
 %, as can be seen in figure \ref{fig:humanPoseLatentSpaces}. 
 In other words, the dynamics disambiguate the model.  
%
%\begin{figure}[ht]
%\begin{center}
%\subfigure[]{ \includegraphics[width=0.18\textwidth]{../diagrams/humanPose/latentSpaceStatic} 
%\label{fig:latentSpaceStatic}
%} \hspace{-3pt}
%\subfigure[]{ \includegraphics[width=0.18\textwidth]{../diagrams/humanPose/latentSpaceDynCropped} 
%\label{fig:latentSpaceDyn}
%}
%\end{center}
%\vspace{-9pt}
%\caption{\small{ \it Projection of the latent space discovered for the non-dynamical model \subref{fig:latentSpaceStatic}
%and for the dynamical model \subref{fig:latentSpaceDyn} onto their two principal dimensions.
%}
%}
%\label{fig:humanPoseLatentSpaces}
%\vspace{-3pt}
%\end{figure}


 Indeed, as can be seen in figure \ref{fig:humanPoseAmbiguityTest},
 our method is forced to select a candidate training input $\bfx_{NN}$
 for initialisation which does not necessarily correspond to the
 training silhouette that is most similar to the test one.
%
%In other words, the predicted pose is generated by a latent point which is not
%necessarily initialised in the latent point that is found by Nearest Neighbour in the silhouette space. 
%
 What is more, if we assume that the test \emph{pose} $\bfzi^*$ is
 known and seek for its nearest training neighbour in the pose space,
 we find that the corresponding silhouette is very similar to the one
 found by our model, which is only given information in the silhouette
 space.

\begin{figure}[ht]
\begin{center}
  \includegraphics[width=0.35\textwidth]{../diagrams/humanPose/ambiguityTest}
\end{center}
\caption{
%\small{\it 
Given the HoG features for the test silhouette in column one, we predict the corresponding pose using the dynamical version of MRD and Nearest Neighbour (NN) in the silhouette space
obtaining the results in the first row, columns 2 and 3 respectively. The last row is the same as the first one, but the poses are rotated
to highlight the ambiguities. Notice that the silhouette shown in the second row for MRD does not correspond exactly to the pose
of the first row, as the model generates only a \emph{novel} pose given a test silhouette. Instead, it is the training silhouette found by performing
NN in the shared latent space. 
The NN of the training \emph{pose} given the test pose is shown in column $4$.
%}
}
\label{fig:humanPoseAmbiguityTest}
\end{figure}

\par Given the above, we quantify the results and compare our method
with linear and Gaussian process regression and Nearest Neighbour in
the silhouette space. We also compared against the shared GP-LVM
\cite{Ek:2008up, Ek:2009vv} which optimises the latent points using
MAP and, therefore, requires an initial factorisation of the inputs to
be given a priori.  Finally, we compared to a dynamical version of Nearest Neighbour where
we kept multiple nearest neighbours and selected the coherent ones over a sequence.
% Specifically,
%for every given test point $\bfy^*_i$ we find the $P$ closest neighbours in the training set and 
%store them in a $P \times D_Y$ matrix $Y^{(nn)}_i$.
%These correspond to a set $Z^{(nn)}_i$ of points in the other view.
%Then, to predict $\bfzi^*_i$ we select the point $\bfzi^{(nn)}_i$ taken from $Z^{(nn)}_i$
% which is closest to $\bfzi^*_{i-1}$, thus forcing an auto-regressive behaviour
%\footnote{We report the error achieved for the best starting pose $\bfzi^*_1$ and the best $P$, taken from a set of possible values $(1,...,10)$.}.
 %
The errors shown in table \ref{humanMotionTable}
as well as the video provided as supplementary material show that MRD
performs better than the other methods in this task.



\begin{table}[h]
\caption{
\label{humanMotionTable}
%\small{ \it
The mean of the Euclidean distances of the joint locations between the predicted and the true poses.
The Nearest Neighbour in the pose space
is not a fair comparison, but is reported here as it provides some insight about the
lower bound on the error that can be achieved for this task.
%}
}
\begin{small}
\begin{center}
\begin{tabular}{ l | l }
%\hline
						 	   & Error \\ \hline %\hline
Mean Training Pose		       & 6.16   \\ %\hline
Linear Regression		       & 5.86   \\ %\hline
GP Regression 			       & 4.27   \\ %\hline
Nearest Neighbour (sil. space) & 4.88  \\ %\hline
Nearest Neighbour with sequences (sil. space) & 4.04  \\ %\hline
\textcolor{light-gray}{Nearest Neighbour (pose space)} & \textcolor{light-gray}{2.08}   \\ %\hline
%Nearest Neighbour (pose space) & 2.08   \\ %\hline %---
Shared GP-LVM				       & 5.13    \\ % \hline
MRD	without Dynamics       & 4.67   \\ %\hline
MRD with Dynamics	       & \textbf{2.94}    \\ \hline
\end{tabular}
\end{center}
\end{small}
\end{table}



%\subsection{Classification}
\noindent{\bf Classification:}
As a final experiment, we demonstrate the flexibility of our model in
a supervised dimensionality reduction scenario for a classification
task. The training dataset was created such that a matrix $Y$
contained the actual observations and a matrix $Z$ the corresponding
class labels in 1-of-K encoding.  We used the `oil' database
\cite{Bishop:oil93} which contains $1000$ $12-$dimensional examples
split in $3$ classes.  We selected $10$ random subsets of the data
with increasing number of training examples and compared to the
nearest neighbor (NN) method in the data space. As can be seen in
figure \ref{fig:oilErrors}, MRD successfully determines the shared
information between the data and the label space and outperforms NN.
%\vspace{-0.2cm}
\begin{figure}[ht]
\begin{center}
  \includegraphics[width=0.4\textwidth]{../diagrams/errorAccuracy}
\end{center}
\vspace{-0.1cm}
\caption{
Accuracy obtained after testing MRD and NN on the full test set of the `oil' dataset.
}
\label{fig:oilErrors}
%\vspace{-0.2cm}
\end{figure}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../svargplvmICML2012"
%%% End: 
