\section{The Model}
Given a two separate data-sets $\mathbf{Y}\in\Re^{N\times D_Y}$ and
$\mathbf{Z}\in\Re^{N\times D_Z}$ we seek a parametrization within the
same model. In specific we assume the existence of a single
low-dimensional latent variable $\mathbf{X}\in\Re^{N\times Q}$ which
through the mappings $f_Y: X \mapsto Y$ and $f_Z: X \mapsto Z$ is
capable of represent the high dimensional data. Assuming the observed
data to have been corrupted by addative gaussian noise,
\begin{align}
  \mathbf{y}_i &= f_Y(\mathbf{x}_i) + \epsilon_i\nonumber\\
  \mathbf{z}_i &= f_Z(\mathbf{x}_i) + \epsilon_i,
\end{align}
we can formulate the likelihood of the data under the model,
$P(\mathbf{Y},\mathbf{Z}|\mathbf{X})$. Finding the latent
representation $\mathbf{X}$ and the mappings $f_Y$ and $f_Z$ is a very
ill-constrained problem with an infinite number of possible
solutions. To provide intuition one analogy would be to think of
meeting a friend in a pub, clearly a nearly infinite combinations of
starting positions and paths exists for your friend to end up at just
this specific pub at this time. Being your good friend, you will have
a reasonably good idea about his whereabouts and habits and from this
information you would be able to limit the number of
possibilities. Very much similar to this analogy the way to proceed is
to exploit priors to regularize the problem.

Of great success in dimensionality reduction has been the idea to
regularize the problem by placing Gaussian Processes
\cite{Rasmussen:2005te} over the generating mapping referred to as a
Gaussian Process Latent Variable Model \cite{Lawrence:2005vk}. The use
of such priors have also been exploited when multiple observation
spaces are availible \cite{Shon:2006wr,Ek:2007uo}. 

%% Our approach seeks to model the underlying, potentially hidden
%% commonality of two (or more) observation spaces $Y$ and $Z$, by
%% assuming that they are both generated by a common latent space $X$ for
%% which the dimensionality and a factorisation is automatically learned.
%% $(Y, Z)$ and $X$ can then be seen as the inputs and outputs
%% respectively of the generative process.  More formally, let $Y \in
%% \mathbb{R}^{N \times D_Y}$, $Z \in \mathbb{R}^{N \times D_Z}$ and $X
%% \in \mathbb{R}^{N \times Q}$ with $D_Y, D_Z \gg Q$.  We use the
%% notation $\bfy_n$, $\bfzi_n$ and $\bfx_n$ respectively to refer to
%% rows of these matrices and $\bfy_d$, $\bfzi_d$, and $\bfx_q$ to refer
%% to columns, and we assume that data are stored by rows.  Obviously,
%% the observed data are allowed to have different dimensionalities but
%% the same number of points, so that each $\bfy_n$ corresponds to a
%% point $\bfzi_n$.


\par Our method builds on the GP-LVM and assumes non-linear generative
mappings $f_Y: X \mapsto Y$ and $f_Z: X \mapsto Z$ from the latent
space to the observation spaces which are learned by a product of
Gaussian Processes (GPs) which act as priors.  

However, unlike
previous MAP-based approaches, \eg \cite{Shon:2006wr}, here we define
a fully Bayesian model which is robust to overfitting and more
flexible. More importantly, it allows for the number of latent
subspaces and their dimensionality to be discovered automatically. To
achieve that, we use the variational Bayesian framework and its
dynamical extension which were recently proposed by Titsias and
Lawrence \cite{Titsias:bayesGPLVM10}, and Damianou \textit{et al.}
\cite{Damianou:vgpds11} respectively.  There, it is shown that a prior
distribution $p(X)$ can be placed in the latent space which can then
be approximately marginalised out, so that the objective function used
in the training procedure is no longer conditioned on $X$. This allows
for \emph{Automatic Relevance Determination (ARD)} priors
\cite{Rasmussen:book06} to be used for the mappings, so that a set of
weights defines the ``importance'' of each latent dimension.

\par Following the idea described above, we use ARD GP priors for our
mappings $f_Y$ and $f_Z$. In that way, we can learn a common latent
space\footnote{Actually we learn a common \emph{distribution} of
  latent points, giving us a set of latent points (mean of the
  distribution) and associated variance.} but we let the two sets of
ARD weights, $\bfw_Y = \{ w^{(q)}_Y \}_{q=1}^Q$ and $\bfw_Z = \{
w^{(q)}_Z \}_{q=1}^Q$ to automatically infer the responsibility of
each latent dimension for generating points in the $Y$ and $Z$ spaces
respectively.  We can then automatically recover a segmentation of the
latent space $X = \left( X_Y, X_s, X_Z \right)$, where $X_s \in
\mathbb{R}^{N \times Q_s}$ is the shared subspace, defined by the set
of dimensions $q \in [1, ... ,Q]$ for which $w_Y^{(q)}, w_Z^{(q)} >
\epsilon$, with $\epsilon$ being a number close to zero. This equips
the model with further flexibility, because
it allows for a softly shared space, if the two sets of weights are
both greater than $\epsilon$ but dissimilar, in general.  As for the
two private spaces, $X_Y$ and $X_Z$, their dimensionalities $Q_Y$ and
$Q_Z$ are also being inferred automatically.  More precisely:
\begin{equation}
X_Y = \{ \bfx_q \}_{q=1}^{Q_Y}: \bfx_q \in X, \; w_Y^{(q)} > \epsilon, \;  w_Z^{(q)} < \epsilon
\end{equation}
and similarly for $X_Z$.  Notice that, in general, there will also be
dimensions of the initial latent space which are considered
unnecessary by both sets of weights. If the subspace corresponding to
these dimensions is denoted with $X_U$, then the actual factorisation
can be written more precisely as $X = \left( X_Y, X_s, X_Z, X_U
\right)$.  All of the above are summarised in graphical model
\ref{fig:grModel}.

%
%\subsection{GP-LVM-based models}
%
%The Gaussian Process Latent Variable Model \cite{Lawrence:2005vk} (GPLVM) is a fully probabilistic, latent variable
% model which enables nonlinear dimensionality reduction and can be seen, therefore, as a more powerful generalisation of
% PPCA. 
%GP-LVM is generative and assumes that the observed $N$ $D$-dimensional datapoints, collectively represented as
% $Y \in \mathbb{R}^{N \times D}$, are generated by a lower dimensional space
% $X \in \mathbb{R}^{N \times Q}$ ($Q \ll D$) via a mapping $f$:
%\begin{equation}
%\label{generative}
%y_{nd} = f_d(\bfx_n) + \epsilon_{nd}, \;\; \epsilon_{nd} \sim \mathcal{N}(0, \beta^{-1}).
%\end{equation}
%Here, $y_{nd}$ denotes the element from the $n$th row and $d$th column of $Y$ and $\epsilon_n$ is a Gaussian noise term.
% Further, we use the notation
%$\bfx_n$ and $\bfy_n$ to refer to rows from $X$ and $Y$ respectively and $x_q$, $y_d$ to refer to columns of these matrices.
%
%The mapping from the latent to the observed space is learned by a product of Gaussian processes which act as a prior.
%%:
%%\begin{equation}
%%\label{gppriorf}
%%f_d(\bfx)  \sim  \mathcal{GP}(0, k_f(\bfx_i,\bfx_j)), \ \ d=1,\ldots,D.
%%\end{equation}
%%where $k_f(\bfx_i,\bfx_j)$ is the covariance function of the GPs, parametrised by a vector of parameters $\bftheta_f$.
%
%\par The standard GP-LVM model defined above, is trained by optimising the likelihood
% $p(Y|X,\bftheta)$, which acts as an objective function. In that way, MAP estimates are found for the model
% parameters $\bftheta$ and the set of latent points $X$. However, conditioning the objective function on $X$ means
% that the number of latent dimensions $Q$ has to be set by hand, or selected non-automatically after exhaustive model selection.
%
%\noindent  \\
%\noindent  \\
%
% \par \textbf{NOTES on the rest}: \\
%\par \textcolor{red}{* 2-3 lines about Bayesian GPLVM and dynamical extension, and the definition of the variational bound
%(which equivalently to the likelihood is the objective function) as $\mathcal{L}(Y) + \text{KL(q(X)||p(X)}$, where the first includes
%the data and the second only the prior, with the link being done by a variational distribution $q(X)$}
%
%\noindent  \\
%
%\subsection{Subspace model}
%
%
%\par \textcolor{red}{* Exploit the fact that the prior and the data exist in different terms, so that we can define a subspace model
%with objective function $\mathcal{L}(Y_1) + \mathcal{L}(Y_2) + ... + \mathcal{L}(Y_M) + \text{KL}(q(X)||p(X)$. 
%Then, $X$ can be shared but thanks to the Bayesian training, the ARD parameters of the cov. function can select subsets of the
%whole $X$. By keeping the sub-models separate and only linking them via the obj. function, we know which model (\ie which dataset $Y_m$)
%selected which subspace of $X$.}
%
%\noindent \\
%
%\par \textcolor{red}{* Or, we can start defining directly the shared model and then explaining GP-LVM and Bayesian GP-LVM on the way...}

\begin{figure*}
  \begin{center}
    \includegraphics[width=0.9\textwidth]{bin/graphicalmodel.pdf}
  \end{center}
  \caption{\small {\it The above figure shows the evolution of the
      topology of the GP-LVM model. On the far left the original model
      is shown, where a single latent variable $\mathbf{X}$ is used to
      represent the observed data $\mathbf{Y}$. }}
 \label{fig:grModel}
\end{figure*}

%% \begin{figure}[ht]
%% \begin{center}
%%   \includegraphics[width=0.18\textwidth]{../diagrams/grModel}
%% \end{center}
%% \vspace{-9pt}
%% \caption{\small{
%% .....}}
%% \label{fig:grModel}
%% \end{figure}

\par The fully Bayesian training procedure requires maximisation of
the logarithm of the joint
\emph{marginal} likelihood $p(Y,Z | \bftheta)$, where $\bftheta$ is the set of model parameters.
Although this quantity is intractable, we can invoke the variational framework of 
\cite{Titsias:bayesGPLVM10,Damianou:vgpds11} and
approximate it with a variational lower bound $\mathcal{F}_v(q,\bftheta) \approx p(Y,Z | \bftheta)$
 which relies on a variational
distribution $q(X)$. This distribution is defined in the same manner as in the \cite{Titsias:bayesGPLVM10,Damianou:vgpds11},
where it is also shown that for a single output space $Y$, the variational lower bound
breaks into two terms 
$\mathcal{L}(Y) + \text{KL}\left[ q(X) || p(X) \right]$, where the first contains
the data $Y$ and the second only involves the prior on $X$. The variational distribution $q(X)$
``binds'' the two terms together. Exploiting this observation for our model which jointly learns
a common (factorised) latent space, allows us to write:
\begin{equation}
\mathcal{F}_v(q,\bftheta) = \mathcal{L}(Y) + \mathcal{L}(Z) + \text{KL} \left[ q(X) || p(X) \right],
\label{eq:bound}
\end{equation}
something which is trivially extended for more than two observed datasets. The above equation is
our final objective function, which is jointly maximised with respect to the model parameters (involving
the mapping parameters for $f_Y$ and $f_Z$ and the corresponding weights $\bfw_Y$ and $\bfw_Z$) and
the variational parameters.

All of the involved terms in the r.h.s of \eqref{eq:bound} are computed exactly as described in \cite{Titsias:bayesGPLVM10,Damianou:vgpds11}.
This means that, as observed in the second of the aforementioned papers, the data are only involved
in the quantities $Y Y^\T$ and $Z Z^\T$ which are $N \times N$ matrices no matter how many features
$D_Y$ and $D_Z$ are used to describe the original data. Also, these quantities are constants and are needed to be
calculated only once. Consequently, our approach is able to model datasets with millions of features.

Further, analogously to \cite{Titsias:bayesGPLVM10,Damianou:vgpds11}, this optimisation procedure simultaneously allows
the variational distribution $q(X)$ to approximate the true posterior $p(X|Y,Z)$, \ie we obtain a distribution
over the latent space. This adds extra robustness to our model, since previous approaches rely on
MAP estimates for the latent points.

%Unlike previous MAP methods, we train our model by using an approximation to the logarithm of
%the joint \textit{marginal} likelihood $\mathcal{F}_v(q, \bftheta) \approx p(Y,Z | \bftheta)$ as an objective function, where
%$\bftheta$ is the set of all model parameters and $q$ refers to the variational distribution
%which renders the approximation tractable. . We exploit the fact that in the variational
%Bayesian framework of \cite{Titsias09} the marginal likelihood $p(Y)$ of a single dataset $Y$
%breaks into two different terms 
%.... \textcolor{red}{bound, training etc.}

%$\mathcal{L}(Y) + \text{KL(q(X)||p(X)}$, where the first includes
%the data and the second only the prior, with the link being done by a variational distribution $q(X)$}

% As shown in \cite{Titsias09}, the variatio
\subsection{Dynamical Modelling}

The model formulation described in the previous section is also covering the case
when we wish to additionally model correlations between datapoints, \ie when
$Y$ and $Z$ are multivariate timeseries. In more detail, for the dynamical scenario
we follow \cite{Damianou:vgpds11,Lawrence:hgplvm07} and choose the prior on the
 latent space to depend on the times 
$\bft \in \mathbb{R}^N$ when the observations were made. In terms of computations, this
 only affects the KL term of equation \eqref{eq:bound}, which can be calculated as described
 in \cite{Damianou:vgpds11}. With this approach, we are also allowed to 
 efficiently model multiple independent sequences which, nevertheless share some commonality. 
This is done by learning a common latent space for all timeseries while, at the same time, ignoring
correlations between datapoints that belong to different sequences.



\subsection{Inference \label{inference}}

Given a model which is trained so as to jointly represent two output spaces $Y$ and $Z$ with
a common but factorised input space $X$, we wish to generate a new (or infer a training) set of outputs
$Z^* \in \mathbb{R}^{N^* \times D_Z}$ given a set of (potentially partially) observed test points $Y^* \in \mathbb{R}^{N^* \times D_Y}$.
This is done in three steps. Firstly, we predict the set of latent points $X^* \in \mathbb{R}^{N^* \times Q}$
which is most likely to have generated $Y^*$. For this, we use an approximation to the posterior $p(X^*|Y^*,Y)$
as is done for the standard Bayesian GP-LVM model \cite{Titsias:bayesGPLVM10,Damianou:vgpds11} and is
similar to the variational distribution $q(X)$ learned during training.
In the second step, we find the training latent points $X_{NN}$ which are closest to $X^*$ in the \emph{shared}
latent space. 
In the third step, we find outputs $Z$ from the GP-LVM likelihood $p(Z | X_{NN})$.

\par The above procedure returns the set of training points $Z$ which best match the observed test points $Y^*$.
In order to generate novel outputs, we have to propagate the information recovered when predicting $X^*$. 
A simple way of doing this is to replace the features of $X_{NN}$ corresponding to the shared latent space,
 with those of $X^*$. This is a reasonable idea, since the shared latent space is supposed to encode the same
 kind of information for both datasets.
 A slightly more sophisticated approach is to also exploit the continuous nature of the
 optimised weights $\bfw_Y$ and give less importance to the dimensions of $X^*$ for which $w_Y^q$ is small,
 because these features are predicted with large uncertainty (variance).
 For example, we can create a new set of latent points $\hat{X}^{*}$ so that its private dimensions match those
 of $X_{NN}$ and its shared dimensions are found by averaging  
the shared dimensions of $X^*$ and $X_{NN}$ 
 as appropriate based on $\bfw_Y$. We can then generate outputs from $p(Z^* | \hat{X}^{*})$.

%We use $C_s$ to refer to the set of indices associated with the dimensions of the latent shared subspace,
%\ie $X_s = \{ \bfx_q\}_{q \in C_s}$.
%\begin{algorithm}
%\caption{Inference for SBGPLVM}
%\label{inferenceAlgorithm}
%\begin{algorithmic}
%\STATE \textbf{Input:} $Y^* = [ \bfy_1^*, ... , \bfy_N^* ], \bfy_n^* \in \mathbb{R}^{D_Y}$
%\STATE ...
%%\REPEAT
%	%\STATE $\forall q$ find $\mathit{S_q^{(k)}}$ from equation \eqref{SFixedPointQ} with $\tilde{\boldsymbol \theta}^{(k-1)}$ being fixed.
%	%\STATE Use a gradient-based method to find
%	%    $\underset{\tilde{\boldsymbol \theta}^{(k)}}{\operatorname{max}} \mathit{F(q, \boldsymbol \theta)}$ 
%	%    from equation \eqref{boundFinal} with $\mathit{S}^{(k)}$ fixed.
%    %\STATE $k = k+1$
%%\UNTIL{convergence}
%\end{algorithmic}
%\end{algorithm}
